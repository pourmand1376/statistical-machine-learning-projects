{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k00pjYR4GrEe"
   },
   "outputs": [],
   "source": [
    "# Amir Pourmand\n",
    "#Kernel shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4flj4Y5bC3cP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats.mstats import gmean\n",
    "from scipy.stats import hmean\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1pTdQ9WHxXh",
    "outputId": "e2767444-b5ed-447f-8608-fff52b5cc293"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfG8u6TCC6yv",
    "outputId": "c0bde923-14ac-41ab-cb1f-b35a8d09fed1"
   },
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.cifar10.load_data()\n",
    "train_data, test_data = data\n",
    "X_train, y_train = train_data\n",
    "X_test, y_test = test_data\n",
    "\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "X_train_prep = preprocess_input(X_train.copy())\n",
    "X_test_prep = preprocess_input(X_test.copy())\n",
    "\n",
    "X_train_prep.shape, X_test_prep.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mXJdjh0zDBVC",
    "outputId": "cd3fa63c-c22a-4965-cae8-fa82012c7787"
   },
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck',\n",
    "}\n",
    "\n",
    "shap_labels = np.array([list(class_names.values())])\n",
    "input_shape = X_train[0].shape\n",
    "num_classes = 10\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        # layers.Flatten(),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EkKypUlD35t"
   },
   "source": [
    "## Implementation Of Shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djdrY3fJH9As"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import inspect\n",
    "import logging\n",
    "import numpy as np\n",
    "import cloudpickle\n",
    "\n",
    "log = logging.getLogger('shap')\n",
    "\n",
    "class Serializable():\n",
    "\n",
    "    def save(self, out_file):\n",
    "        pickle.dump(type(self), out_file)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, in_file, instantiate=True):\n",
    "        if instantiate:\n",
    "            return cls._instantiated_load(in_file)\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def _instantiated_load(cls, in_file, **kwargs):\n",
    "        obj_type = pickle.load(in_file)\n",
    "        if obj_type is None:\n",
    "            return None\n",
    "\n",
    "        if not inspect.isclass(obj_type) or (not issubclass(obj_type, cls) and (obj_type is not cls)):\n",
    "            raise Exception(f\"Invalid object type loaded from file. {obj_type} is not a subclass of {cls}.\")\n",
    "\n",
    "        # here we call the constructor with all the arguments we have loaded\n",
    "        constructor_args = obj_type.load(in_file, instantiate=False, **kwargs)\n",
    "        used_args = inspect.getfullargspec(obj_type.__init__)[0]\n",
    "        return obj_type(**{k: constructor_args[k] for k in constructor_args if k in used_args})\n",
    "\n",
    "\n",
    "class Serializer():\n",
    "    def __init__(self, out_stream, block_name, version):\n",
    "        self.out_stream = out_stream\n",
    "        self.block_name = block_name\n",
    "        self.block_version = version\n",
    "        self.serializer_version = 0 # update this when the serializer changes\n",
    "\n",
    "    def __enter__(self):\n",
    "        log.debug(\"serializer_version = %d\", self.serializer_version)\n",
    "        pickle.dump(self.serializer_version, self.out_stream)\n",
    "        log.debug(\"block_name = %s\", self.block_name)\n",
    "        pickle.dump(self.block_name, self.out_stream)\n",
    "        log.debug(\"block_version = %d\", self.block_version)\n",
    "        pickle.dump(self.block_version, self.out_stream)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        log.debug(\"END_BLOCK___\")\n",
    "        pickle.dump(\"END_BLOCK___\", self.out_stream)\n",
    "\n",
    "    def save(self, name, value, encoder=\"auto\"):\n",
    "        log.debug(\"name = %s\", name)\n",
    "        pickle.dump(name, self.out_stream)\n",
    "        if encoder is None or encoder is False:\n",
    "            log.debug(\"encoder_name = %s\", \"no_encoder\")\n",
    "            pickle.dump(\"no_encoder\", self.out_stream)\n",
    "        elif callable(encoder):\n",
    "            log.debug(\"encoder_name = %s\", \"custom_encoder\")\n",
    "            pickle.dump(\"custom_encoder\", self.out_stream)\n",
    "            encoder(value, self.out_stream)\n",
    "        elif encoder == \".save\" or (isinstance(value, Serializable) and encoder == \"auto\"):\n",
    "            log.debug(\"encoder_name = %s\", \"serializable.save\")\n",
    "            pickle.dump(\"serializable.save\", self.out_stream)\n",
    "            if len(inspect.getfullargspec(value.save)[0]) == 3: # backward compat for MLflow, can remove 4/1/2021\n",
    "                value.save(self.out_stream, value)\n",
    "            else:\n",
    "                value.save(self.out_stream)\n",
    "        elif encoder == \"auto\":\n",
    "            if isinstance(value, (int, float, str)):\n",
    "                log.debug(\"encoder_name = %s\", \"pickle.dump\")\n",
    "                pickle.dump(\"pickle.dump\", self.out_stream)\n",
    "                pickle.dump(value, self.out_stream)\n",
    "            else:\n",
    "                log.debug(\"encoder_name = %s\", \"cloudpickle.dump\")\n",
    "                pickle.dump(\"cloudpickle.dump\", self.out_stream)\n",
    "                cloudpickle.dump(value, self.out_stream)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown encoder type '{encoder}' given for serialization!\")\n",
    "        log.debug(\"value = %s\", str(value))\n",
    "\n",
    "class Deserializer():\n",
    "    \"\"\" Load data items from an input stream.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_stream, block_name, min_version, max_version):\n",
    "        self.in_stream = in_stream\n",
    "        self.block_name = block_name\n",
    "        self.block_min_version = min_version\n",
    "        self.block_max_version = max_version\n",
    "\n",
    "        # update these when the serializer changes\n",
    "        self.serializer_min_version = 0\n",
    "        self.serializer_max_version = 0\n",
    "\n",
    "    def __enter__(self):\n",
    "\n",
    "        # confirm the serializer version\n",
    "        serializer_version = pickle.load(self.in_stream)\n",
    "        log.debug(\"serializer_version = %d\", serializer_version)\n",
    "        if serializer_version < self.serializer_min_version:\n",
    "            raise ValueError(\n",
    "                f\"The file being loaded was saved with a serializer version of {serializer_version}, \" + \\\n",
    "                f\"but the current deserializer in SHAP requires at least version {self.serializer_min_version}.\"\n",
    "            )\n",
    "        if serializer_version > self.serializer_max_version:\n",
    "            raise ValueError(\n",
    "                f\"The file being loaded was saved with a serializer version of {serializer_version}, \" + \\\n",
    "                f\"but the current deserializer in SHAP only support up to version {self.serializer_max_version}.\"\n",
    "            )\n",
    "\n",
    "        # confirm the block name\n",
    "        block_name = pickle.load(self.in_stream)\n",
    "        log.debug(\"block_name = %s\", block_name)\n",
    "        if block_name != self.block_name:\n",
    "            raise ValueError(\n",
    "                f\"The next data block in the file being loaded was supposed to be {self.block_name}, \" + \\\n",
    "                f\"but the next block found was {block_name}.\"\n",
    "            )\n",
    "\n",
    "        # confirm the block version\n",
    "        block_version = pickle.load(self.in_stream)\n",
    "        log.debug(\"block_version = %d\", block_version)\n",
    "        if block_version < self.block_min_version:\n",
    "            raise ValueError(\n",
    "                f\"The file being loaded was saved with a block version of {block_version}, \" + \\\n",
    "                f\"but the current deserializer in SHAP requires at least version {self.block_min_version}.\"\n",
    "            )\n",
    "        if block_version > self.block_max_version:\n",
    "            raise ValueError(\n",
    "                f\"The file being loaded was saved with a block version of {block_version}, \" + \\\n",
    "                f\"but the current deserializer in SHAP only support up to version {self.block_max_version}.\"\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        # confirm the block end token\n",
    "        for _ in range(100):\n",
    "            end_token = pickle.load(self.in_stream)\n",
    "            log.debug(\"end_token = %s\", end_token)\n",
    "            if end_token == \"END_BLOCK___\":\n",
    "                return\n",
    "            else:\n",
    "                self._load_data_value()\n",
    "        raise ValueError(\n",
    "            f\"The data block end token wsa not found for the block {self.block_name}.\"\n",
    "        )\n",
    "\n",
    "    def load(self, name, decoder=None):\n",
    "        \"\"\" Load a data item from the current input stream.\n",
    "        \"\"\"\n",
    "        # confirm the block name\n",
    "        loaded_name = pickle.load(self.in_stream)\n",
    "        log.debug(\"loaded_name = %s\", loaded_name)\n",
    "        print(\"loaded_name\", loaded_name)\n",
    "        if loaded_name != name:\n",
    "            raise ValueError(\n",
    "                f\"The next data item in the file being loaded was supposed to be {name}, \" + \\\n",
    "                f\"but the next block found was {loaded_name}.\"\n",
    "            ) # We should eventually add support for skipping over unused data items in old formats...\n",
    "\n",
    "        value = self._load_data_value(decoder)\n",
    "        log.debug(\"value = %s\", str(value))\n",
    "        return value\n",
    "\n",
    "    def _load_data_value(self, decoder=None):\n",
    "        encoder_name = pickle.load(self.in_stream)\n",
    "        log.debug(\"encoder_name = %s\", encoder_name)\n",
    "        if encoder_name == \"custom_encoder\" or callable(decoder):\n",
    "            assert callable(decoder), \"You must provide a callable custom decoder for the data item {name}!\"\n",
    "            return decoder(self.in_stream)\n",
    "        if encoder_name == \"no_encoder\":\n",
    "            return None\n",
    "        if encoder_name == \"serializable.save\":\n",
    "            return Serializable.load(self.in_stream)\n",
    "        if encoder_name == \"numpy.save\":\n",
    "            return np.load(self.in_stream)\n",
    "        if encoder_name == \"pickle.dump\":\n",
    "            return pickle.load(self.in_stream)\n",
    "        if encoder_name == \"cloudpickle.dump\":\n",
    "            return cloudpickle.load(self.in_stream)\n",
    "\n",
    "        raise ValueError(f\"Unsupported encoder type found: {encoder_name}\")\n",
    "{\"mode\":\"full\",\"isActive\":False}\n",
    "\n",
    "class Masker(Serializable):\n",
    "\n",
    "    def __call__(self, mask, *args):\n",
    "        '''super class of all maskers'''\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0mq_wUKHsaI"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numba\n",
    "\n",
    "@numba.jit\n",
    "def identity(x):\n",
    "    return x\n",
    "@numba.jit\n",
    "def _identity_inverse(x):\n",
    "    return x\n",
    "identity.inverse = _identity_inverse\n",
    "\n",
    "@numba.jit\n",
    "def logit(x):\n",
    "    return np.log(x/(1-x))\n",
    "@numba.jit\n",
    "def _logit_inverse(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "logit.inverse = _logit_inverse\n",
    "\n",
    "def safe_isinstance(obj, class_path_str):\n",
    "    if isinstance(class_path_str, str):\n",
    "        class_path_strs = [class_path_str]\n",
    "    elif isinstance(class_path_str, list) or isinstance(class_path_str, tuple):\n",
    "        class_path_strs = class_path_str\n",
    "    else:\n",
    "        class_path_strs = ['']\n",
    "    \n",
    "    # try each module path in order\n",
    "    for class_path_str in class_path_strs:\n",
    "        if \".\" not in class_path_str:\n",
    "            raise ValueError(\"class_path_str must be a string or list of strings specifying a full \\\n",
    "                module path to a class. Eg, 'sklearn.ensemble.RandomForestRegressor'\")\n",
    "\n",
    "        module_name, class_name = class_path_str.rsplit(\".\", 1)\n",
    "\n",
    "        if module_name not in sys.modules:\n",
    "            continue\n",
    "\n",
    "        module = sys.modules[module_name]\n",
    "        \n",
    "        #Get class\n",
    "        _class = getattr(module, class_name, None)\n",
    "        \n",
    "        if _class is None:\n",
    "            continue\n",
    "        \n",
    "        if isinstance(obj, _class):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class ShowProgress():\n",
    "    def __init__(self, iterable, total, desc, silent, start_delay):\n",
    "        self.iter = iter(iterable)\n",
    "        self.start_time = time.time()\n",
    "        self.pbar = None\n",
    "        self.total = total\n",
    "        self.desc = desc\n",
    "        self.start_delay = start_delay\n",
    "        self.silent = silent\n",
    "        self.unshown_count = 0\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.pbar is None and time.time() - self.start_time > self.start_delay:\n",
    "           self.pbar = tqdm.tqdm(total=self.total, initial=self.unshown_count, desc=self.desc, disable=self.silent)\n",
    "           self.pbar.start_t = self.start_time\n",
    "        if self.pbar is not None:\n",
    "            self.pbar.update(1)\n",
    "        else:\n",
    "            self.unshown_count += 1\n",
    "        try:\n",
    "            return next(self.iter)\n",
    "        except StopIteration as e:\n",
    "            if self.pbar is not None:\n",
    "                self.pbar.close()\n",
    "            raise e\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "def show_progress(iterable, total=None, desc=None, silent=False, start_delay=10):\n",
    "    return ShowProgress(iterable, total, desc, silent, start_delay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkP6o1WqKrcw"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Model(Serializable):\n",
    "\n",
    "    def __init__(self, model=None):\n",
    "        \"\"\" Wrap a callable model as a SHAP Model object.\n",
    "        \"\"\"\n",
    "        if isinstance(model, Model):\n",
    "            self.inner_model = model.inner_model\n",
    "        else:\n",
    "            self.inner_model = model\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        return self.inner_model(*args)\n",
    "\n",
    "    def save(self, out_file):\n",
    "        \"\"\" Save the model to the given file stream.\n",
    "        \"\"\"\n",
    "        super().save(out_file)\n",
    "        with Serializer(out_file, \"shap.Model\", version=0) as s:\n",
    "            s.save(\"model\", self.inner_model)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, in_file, instantiate=True):\n",
    "        if instantiate:\n",
    "            return cls._instantiated_load(in_file)\n",
    "\n",
    "        kwargs = super().load(in_file, instantiate=False)\n",
    "        with Deserializer(in_file, \"shap.Model\", min_version=0, max_version=0) as s:\n",
    "            kwargs[\"model\"] = s.load(\"model\")\n",
    "        return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92uQGoLmK4rS"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TeacherForcing(Model):\n",
    "\n",
    "    def __init__(self, model, tokenizer=None, similarity_model=None, similarity_tokenizer=None, batch_size=128, device=None):\n",
    "        super().__init__(model)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        # set pad token if not defined\n",
    "        if self.tokenizer is not None and self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        # assign text generation function\n",
    "        if safe_isinstance(model, \"transformers.PreTrainedModel\") or safe_isinstance(model, \"transformers.TFPreTrainedModel\"):\n",
    "            self.text_generate = models.TextGeneration(self.inner_model, tokenizer=self.tokenizer, device=self.device)\n",
    "            self.similarity_model = model\n",
    "            self.similarity_tokenizer = tokenizer\n",
    "            self.model_agnostic = False\n",
    "        else:\n",
    "            self.text_generate = models.TextGeneration(self.inner_model, device=self.device)\n",
    "            self.similarity_model = similarity_model\n",
    "            self.similarity_tokenizer = similarity_tokenizer\n",
    "            # set pad token for a similarity tokenizer(in a model agnostic scenario) if not defined\n",
    "            if self.similarity_tokenizer is not None and self.similarity_tokenizer.pad_token is None:\n",
    "                self.similarity_tokenizer.pad_token = self.similarity_tokenizer.eos_token\n",
    "            self.model_agnostic = True\n",
    "        # initializing target which is the target sentence/ids for every new row of explanation\n",
    "        self.output = None\n",
    "        self.output_names = None\n",
    "\n",
    "        self.similarity_model_type = None\n",
    "        if safe_isinstance(self.similarity_model, \"transformers.PreTrainedModel\"):\n",
    "            self.similarity_model_type = \"pt\"\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if self.device is None else self.device\n",
    "            self.similarity_model = self.similarity_model.to(self.device)\n",
    "        elif safe_isinstance(self.similarity_model, \"transformers.TFPreTrainedModel\"):\n",
    "            self.similarity_model_type = \"tf\"\n",
    "\n",
    "    def __call__(self, X, Y):\n",
    "        output_batch = None\n",
    "        # caching updates output names and target sentence ids\n",
    "        self.update_output_names(Y[:1])\n",
    "        start_batch_idx, end_batch_idx = 0, len(X)\n",
    "        while start_batch_idx < end_batch_idx:\n",
    "            X_batch = X[start_batch_idx:start_batch_idx+self.batch_size]\n",
    "            Y_batch = Y[start_batch_idx:start_batch_idx+self.batch_size]\n",
    "            logits = self.get_teacher_forced_logits(X_batch, Y_batch)\n",
    "            logodds = self.get_logodds(logits)\n",
    "            if output_batch is None:\n",
    "                output_batch = logodds\n",
    "            else:\n",
    "                output_batch = np.concatenate((output_batch, logodds))\n",
    "            start_batch_idx += self.batch_size\n",
    "        return output_batch\n",
    "\n",
    "    def update_output_names(self, output):\n",
    "        # check if the target sentence has been updated (occurs when explaining a new row)\n",
    "        if (self.output is None) or (not np.array_equal(self.output, output)):\n",
    "            self.output = output\n",
    "            self.output_names = self.get_output_names(output)\n",
    "\n",
    "    def get_output_names(self, output):\n",
    "        output_ids = self.get_outputs(output)\n",
    "        output_names = [self.similarity_tokenizer.decode([x]).strip() for x in output_ids[0, :]]\n",
    "        return output_names\n",
    "\n",
    "    def get_outputs(self, X):\n",
    "        # check if output is a sentence or already parsed target ids\n",
    "        if X.dtype.type is np.str_:\n",
    "            parsed_tokenizer_dict = parse_prefix_suffix_for_tokenizer(self.similarity_tokenizer)\n",
    "            keep_prefix, keep_suffix = parsed_tokenizer_dict['keep_prefix'], parsed_tokenizer_dict['keep_suffix']\n",
    "            if keep_suffix > 0:\n",
    "                output_ids = np.array(self.similarity_tokenizer(X.tolist(), padding=True)[\"input_ids\"])[:, keep_prefix:-keep_suffix]\n",
    "            else:\n",
    "                output_ids = np.array(self.similarity_tokenizer(X.tolist(), padding=True)[\"input_ids\"])[:, keep_prefix:]\n",
    "        else:\n",
    "            output_ids = X\n",
    "        return output_ids\n",
    "\n",
    "    def get_inputs(self, X, padding_side='right'):\n",
    "        if self.model_agnostic:\n",
    "            input_sentences = np.array(self.inner_model(X))\n",
    "        else:\n",
    "            input_sentences = np.array(X)\n",
    "        self.similarity_tokenizer.padding_side = padding_side\n",
    "        inputs = self.similarity_tokenizer(input_sentences.tolist(), return_tensors=self.similarity_model_type, padding=True)\n",
    "        self.similarity_tokenizer.padding_side = 'right'\n",
    "        return inputs\n",
    "\n",
    "    def get_logodds(self, logits):\n",
    "        if self.output.dtype.type is np.str_:\n",
    "            output_ids = self.get_outputs(self.output)[0]\n",
    "        else:\n",
    "            output_ids = self.output[0]\n",
    "\n",
    "        def calc_logodds(arr):\n",
    "            probs = np.exp(arr) / np.exp(arr).sum(-1)\n",
    "            logodds = sp.special.logit(probs)\n",
    "            return logodds\n",
    "\n",
    "        logodds = np.apply_along_axis(calc_logodds, -1, logits)\n",
    "        logodds_for_output_ids = logodds[:, np.array(range(logodds.shape[1])), output_ids]\n",
    "        return logodds_for_output_ids\n",
    "\n",
    "    def model_inference(self, inputs, output_ids):\n",
    "        if self.similarity_model_type == \"pt\":\n",
    "            inputs = inputs.to(self.device)\n",
    "            output_ids = torch.tensor(output_ids, dtype=torch.int64, device=self.device)\n",
    "            self.similarity_model.eval()\n",
    "            with torch.no_grad():\n",
    "                if self.similarity_model.config.is_encoder_decoder:\n",
    "                    outputs = self.similarity_model(**inputs, decoder_input_ids=output_ids, labels=output_ids, return_dict=True)\n",
    "                else:\n",
    "                    inputs[\"input_ids\"] = torch.cat((inputs[\"input_ids\"], output_ids), dim=-1)\n",
    "                    attention_mask_for_output_ids = torch.ones(output_ids.shape, dtype=output_ids.dtype, device=self.device)\n",
    "                    inputs[\"attention_mask\"] = torch.cat((inputs[\"attention_mask\"], attention_mask_for_output_ids), dim=-1)\n",
    "                    inputs[\"position_ids\"] = (inputs[\"attention_mask\"].long().cumsum(-1) - 1)\n",
    "                    inputs[\"position_ids\"].masked_fill_(inputs[\"attention_mask\"] == 0, 0)\n",
    "                    outputs = self.similarity_model(**inputs, return_dict=True)\n",
    "                logits = outputs.logits.detach().cpu().numpy().astype('float64')\n",
    "        elif self.similarity_model_type == \"tf\":\n",
    "            output_ids = tf.convert_to_tensor(output_ids, dtype=tf.int32)\n",
    "            if self.similarity_model.config.is_encoder_decoder:\n",
    "                if self.device is None:\n",
    "                    outputs = self.similarity_model(inputs, decoder_input_ids=output_ids, labels=output_ids, return_dict=True)\n",
    "                else:\n",
    "                    try:\n",
    "                        with tf.device(self.device):\n",
    "                            outputs = self.similarity_model(inputs, decoder_input_ids=output_ids, labels=output_ids, return_dict=True)\n",
    "                    except RuntimeError as e:\n",
    "                        print(e)\n",
    "            else:\n",
    "                inputs[\"input_ids\"] = tf.concat((inputs[\"input_ids\"], output_ids), axis=-1)\n",
    "                attention_mask_for_output_ids = tf.ones(output_ids.shape, dtype=output_ids.dtype)\n",
    "                inputs[\"attention_mask\"] = tf.concat((inputs[\"attention_mask\"], attention_mask_for_output_ids), axis=-1)\n",
    "                inputs[\"position_ids\"] = tf.math.cumsum(inputs[\"attention_mask\"], axis=-1) - 1\n",
    "                inputs[\"position_ids\"] = tf.where(inputs[\"attention_mask\"] == 0, 0, inputs[\"position_ids\"])\n",
    "                if self.device is None:\n",
    "                    outputs = self.similarity_model(inputs, return_dict=True)\n",
    "                else:\n",
    "                    try:\n",
    "                        with tf.device(self.device):\n",
    "                            outputs = self.similarity_model(inputs, return_dict=True)\n",
    "                    except RuntimeError as e:\n",
    "                        print(e)\n",
    "            logits = outputs.logits.numpy().astype('float64')\n",
    "        return logits\n",
    "\n",
    "    def get_teacher_forced_logits(self, X, Y):\n",
    "        if (hasattr(self.similarity_model.config, \"is_encoder_decoder\") and not self.similarity_model.config.is_encoder_decoder) \\\n",
    "            and (hasattr(self.similarity_model.config, \"is_decoder\") and not self.similarity_model.config.is_decoder):\n",
    "            raise ValueError(\n",
    "                \"Please assign either of is_encoder_decoder or is_decoder to True in model config for extracting target sentence ids\"\n",
    "            )\n",
    "        output_ids = self.get_outputs(Y)\n",
    "        if self.similarity_model.config.is_encoder_decoder:\n",
    "            inputs = self.get_inputs(X, padding_side='right')\n",
    "            decoder_start_token_id = None\n",
    "            if hasattr(self.similarity_model.config, \"decoder_start_token_id\") and \\\n",
    "                    self.similarity_model.config.decoder_start_token_id is not None:\n",
    "                decoder_start_token_id = self.similarity_model.config.decoder_start_token_id\n",
    "            elif hasattr(self.similarity_model.config, \"bos_token_id\") and self.similarity_model.config.bos_token_id is not None:\n",
    "                decoder_start_token_id = self.similarity_model.config.bos_token_id\n",
    "            elif (hasattr(self.similarity_model.config, \"decoder\") and hasattr(self.similarity_model.config.decoder, \"bos_token_id\") and \\\n",
    "                    self.similarity_model.config.decoder.bos_token_id is not None):\n",
    "                decoder_start_token_id = self.similarity_model.config.decoder.bos_token_id\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"No decoder_start_token_id or bos_token_id defined in config for encoder-decoder generation\"\n",
    "                )\n",
    "            output_start_id = np.ones((output_ids.shape[0], 1)) * decoder_start_token_id\n",
    "            output_ids = np.concatenate((output_start_id, output_ids), axis=-1)\n",
    "            logits = self.model_inference(inputs, output_ids)\n",
    "            logits = logits[:, :-1, :]\n",
    "        else:\n",
    "            inputs = self.get_inputs(X, padding_side='left')\n",
    "            logits = self.model_inference(inputs, output_ids)\n",
    "            logits = logits[:, -output_ids.shape[1]-1:-1, :]\n",
    "        return logits\n",
    "\n",
    "    def save(self, out_file):\n",
    "        super().save(out_file)\n",
    "\n",
    "        with Serializer(out_file, \"shap.models.TeacherForcing\", version=0) as s:\n",
    "            s.save(\"tokenizer\", self.tokenizer)\n",
    "            s.save(\"similarity_model\", self.similarity_model)\n",
    "            s.save(\"similarity_tokenizer\", self.similarity_tokenizer)\n",
    "            s.save(\"batch_size\", self.batch_size)\n",
    "            s.save(\"device\", self.device)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, in_file, instantiate=True):\n",
    "        if instantiate:\n",
    "            return cls._instantiated_load(in_file)\n",
    "\n",
    "        kwargs = super().load(in_file, instantiate=False)\n",
    "        with Deserializer(in_file, \"shap.models.TeacherForcing\", min_version=0, max_version=0) as s:\n",
    "            kwargs[\"tokenizer\"] = s.load(\"tokenizer\")\n",
    "            kwargs[\"similarity_model\"] = s.load(\"similarity_model\")\n",
    "            kwargs[\"similarity_tokenizer\"] = s.load(\"similarity_tokenizer\")\n",
    "            kwargs[\"batch_size\"] = s.load(\"batch_size\")\n",
    "            kwargs[\"device\"] = s.load(\"device\")\n",
    "        return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPeW_O-XEYEj"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "class Explainer(Serializable):\n",
    "\n",
    "    def __init__(self, model, masker=None, link=None, algorithm=\"auto\", output_names=None, feature_names=None, **kwargs):\n",
    "        self.model = model\n",
    "        self.output_names = output_names\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "        # wrap the incoming masker object as a shap.Masker object\n",
    "        if safe_isinstance(masker, \"pandas.core.frame.DataFrame\") or \\\n",
    "                ((safe_isinstance(masker, \"numpy.ndarray\") or sp.sparse.issparse(masker)) and len(masker.shape) == 2):\n",
    "            if algorithm == \"partition\":\n",
    "                self.masker = maskers.Partition(masker)\n",
    "            else:\n",
    "                self.masker = maskers.Independent(masker)\n",
    "        elif safe_isinstance(masker, [\"transformers.PreTrainedTokenizer\", \"transformers.tokenization_utils_base.PreTrainedTokenizerBase\"]):\n",
    "            if (safe_isinstance(self.model, \"transformers.PreTrainedModel\") or safe_isinstance(self.model, \"transformers.TFPreTrainedModel\")) and \\\n",
    "                    safe_isinstance(self.model, MODELS_FOR_SEQ_TO_SEQ_CAUSAL_LM + MODELS_FOR_CAUSAL_LM):\n",
    "                self.masker = maskers.Text(masker, mask_token=\"...\", collapse_mask_token=True)\n",
    "            else:\n",
    "                self.masker = maskers.Text(masker)\n",
    "        elif (masker is list or masker is tuple) and masker[0] is not str:\n",
    "            self.masker = maskers.Composite(*masker)\n",
    "        elif (masker is dict) and (\"mean\" in masker):\n",
    "            self.masker = maskers.Independent(masker)\n",
    "        elif masker is None and isinstance(self.model, models.TransformersPipeline):\n",
    "            return self.__init__( # pylint: disable=non-parent-init-called\n",
    "                self.model, self.model.inner_model.tokenizer,\n",
    "                link=link, algorithm=algorithm, output_names=output_names, feature_names=feature_names, **kwargs\n",
    "            )\n",
    "        else:\n",
    "            self.masker = masker\n",
    "\n",
    "        if safe_isinstance(self.model, \"transformers.pipelines.Pipeline\"):\n",
    "            return self.__init__( \n",
    "                models.TransformersPipeline(self.model), self.masker,\n",
    "                link=link, algorithm=algorithm, output_names=output_names, feature_names=feature_names, **kwargs\n",
    "            )\n",
    "\n",
    "        if (safe_isinstance(self.model, \"transformers.PreTrainedModel\") or safe_isinstance(self.model, \"transformers.TFPreTrainedModel\")) and \\\n",
    "                safe_isinstance(self.model, MODELS_FOR_SEQ_TO_SEQ_CAUSAL_LM + MODELS_FOR_CAUSAL_LM):\n",
    "            self.model = models.TeacherForcing(self.model, self.masker.tokenizer)\n",
    "            self.masker = maskers.OutputComposite(self.masker, self.model.text_generate)\n",
    "        elif safe_isinstance(self.model, \"shap.models.TeacherForcing\") and safe_isinstance(self.masker, [\"shap.maskers.Text\", \"shap.maskers.Image\"]):\n",
    "            self.masker = maskers.OutputComposite(self.masker, self.model.text_generate)\n",
    "        elif safe_isinstance(self.model, \"shap.models.TopKLM\") and safe_isinstance(self.masker, \"shap.maskers.Text\"):\n",
    "            self.masker = maskers.FixedComposite(self.masker)\n",
    "\n",
    "        if callable(link) and callable(getattr(link, \"inverse\", None)):\n",
    "            self.link = link\n",
    "        else:\n",
    "            raise Exception(\"The passed link function needs to be callable and have a callable .inverse property!\")\n",
    "\n",
    "        if self.__class__ is Explainer:\n",
    "\n",
    "            if algorithm == \"auto\":\n",
    "\n",
    "                if explainers.Linear.supports_model_with_masker(model, self.masker):\n",
    "                    algorithm = \"linear\"\n",
    "                elif explainers.Tree.supports_model_with_masker(model, self.masker): \n",
    "                    algorithm = \"tree\"\n",
    "                elif explainers.Additive.supports_model_with_masker(model, self.masker):\n",
    "                    algorithm = \"additive\"\n",
    "\n",
    "                elif callable(self.model):\n",
    "                    if issubclass(type(self.masker), maskers.Independent):\n",
    "                        if self.masker.shape[1] <= 10:\n",
    "                            algorithm = \"exact\"\n",
    "                        else:\n",
    "                            algorithm = \"permutation\"\n",
    "                    elif issubclass(type(self.masker), maskers.Partition):\n",
    "                        if self.masker.shape[1] <= 32:\n",
    "                            algorithm = \"exact\"\n",
    "                        else:\n",
    "                            algorithm = \"permutation\"\n",
    "                    elif issubclass(type(self.masker), maskers.Composite):\n",
    "                        if getattr(self.masker, \"partition_tree\", None) is None:\n",
    "                            algorithm = \"permutation\"\n",
    "                        else:\n",
    "                            algorithm = \"partition\" # TODO: should really only do this if there is more than just tabular\n",
    "                    elif issubclass(type(self.masker), maskers.Image) or issubclass(type(self.masker), maskers.Text) or \\\n",
    "                            issubclass(type(self.masker), maskers.OutputComposite) or issubclass(type(self.masker), maskers.FixedComposite):\n",
    "                        algorithm = \"partition\"\n",
    "                    else:\n",
    "                        algorithm = \"permutation\"\n",
    "\n",
    "                # if we get here then we don't know how to handle what was given to us\n",
    "                else:\n",
    "                    raise Exception(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: \" + str(model))\n",
    "\n",
    "            # build the right subclass\n",
    "            if algorithm == \"exact\":\n",
    "                self.__class__ = explainers.Exact\n",
    "                explainers.Exact.__init__(self, self.model, self.masker, link=self.link, feature_names=self.feature_names, **kwargs)\n",
    "            elif algorithm == \"permutation\":\n",
    "                self.__class__ = explainers.Permutation\n",
    "                explainers.Permutation.__init__(self, self.model, self.masker, link=self.link, feature_names=self.feature_names, **kwargs)\n",
    "            elif algorithm == \"partition\":\n",
    "                self.__class__ = explainers.Partition\n",
    "                explainers.Partition.__init__(self, self.model, self.masker, link=self.link, feature_names=self.feature_names, output_names=self.output_names, **kwargs)\n",
    "            elif algorithm == \"tree\":\n",
    "                self.__class__ = explainers.Tree\n",
    "                explainers.Tree.__init__(self, self.model, self.masker, link=self.link, feature_names=self.feature_names, **kwargs)\n",
    "            elif algorithm == \"additive\":\n",
    "                self.__class__ = explainers.Additive\n",
    "                explainers.Additive.__init__(self, self.model, self.masker, link=self.link, feature_names=self.feature_names, **kwargs)\n",
    "            elif algorithm == \"linear\":\n",
    "                self.__class__ = explainers.Linear\n",
    "                explainers.Linear.__init__(self, self.model, self.masker, link=self.link, feature_names=self.feature_names, **kwargs)\n",
    "            else:\n",
    "                raise Exception(\"Unknown algorithm type passed: %s!\" % algorithm)\n",
    "\n",
    "\n",
    "    def __call__(self, *args, max_evals=\"auto\", main_effects=False, error_bounds=False, batch_size=\"auto\",\n",
    "                 outputs=None, silent=False, **kwargs):\n",
    "      \n",
    "        if issubclass(type(self.masker), maskers.OutputComposite) and len(args)==2:\n",
    "            self.masker.model = models.TextGeneration(target_sentences=args[1])\n",
    "            args = args[:1]\n",
    "        num_rows = None\n",
    "        args = list(args)\n",
    "        if self.feature_names is None:\n",
    "            feature_names = [None for _ in range(len(args))]\n",
    "        elif issubclass(type(self.feature_names[0]), (list, tuple)):\n",
    "            feature_names = copy.deepcopy(self.feature_names)\n",
    "        else:\n",
    "            feature_names = [copy.deepcopy(self.feature_names)]\n",
    "        for i in range(len(args)):\n",
    "\n",
    "           \n",
    "            if num_rows is None:\n",
    "                try:\n",
    "                    num_rows = len(args[i])\n",
    "                except Exception: \n",
    "                    pass\n",
    "\n",
    "            if safe_isinstance(args[i], \"pandas.core.frame.DataFrame\"):\n",
    "                feature_names[i] = list(args[i].columns)\n",
    "                args[i] = args[i].to_numpy()\n",
    "\n",
    "            if safe_isinstance(args[i], \"nlp.arrow_dataset.Dataset\"):\n",
    "                args[i] = args[i][\"text\"]\n",
    "            elif issubclass(type(args[i]), dict) and \"text\" in args[i]:\n",
    "                args[i] = args[i][\"text\"]\n",
    "\n",
    "        if batch_size == \"auto\":\n",
    "            if hasattr(self.masker, \"default_batch_size\"):\n",
    "                batch_size = self.masker.default_batch_size\n",
    "            else:\n",
    "                batch_size = 10\n",
    "\n",
    "        values = []\n",
    "        output_indices = []\n",
    "        expected_values = []\n",
    "        mask_shapes = []\n",
    "        main_effects = []\n",
    "        hierarchical_values = []\n",
    "        clustering = []\n",
    "        output_names = []\n",
    "        if callable(getattr(self.masker, \"feature_names\", None)):\n",
    "            feature_names = [[] for _ in range(len(args))]\n",
    "        for row_args in show_progress(zip(*args), num_rows, self.__class__.__name__+\" explainer\", silent):\n",
    "            row_result = self.explain_row(\n",
    "                *row_args, max_evals=max_evals, main_effects=main_effects, error_bounds=error_bounds,\n",
    "                batch_size=batch_size, outputs=outputs, silent=silent, **kwargs\n",
    "            )\n",
    "            values.append(row_result.get(\"values\", None))\n",
    "            output_indices.append(row_result.get(\"output_indices\", None))\n",
    "            expected_values.append(row_result.get(\"expected_values\", None))\n",
    "            mask_shapes.append(row_result[\"mask_shapes\"])\n",
    "            main_effects.append(row_result.get(\"main_effects\", None))\n",
    "            clustering.append(row_result.get(\"clustering\", None))\n",
    "            hierarchical_values.append(row_result.get(\"hierarchical_values\", None))\n",
    "            output_names.append(row_result.get(\"output_names\", None))\n",
    "\n",
    "            if callable(getattr(self.masker, \"feature_names\", None)):\n",
    "                row_feature_names = self.masker.feature_names(*row_args)\n",
    "                for i in range(len(row_args)):\n",
    "                    feature_names[i].append(row_feature_names[i])\n",
    "\n",
    "        arg_values = [[] for a in args]\n",
    "        for i, v in enumerate(values):\n",
    "            pos = 0\n",
    "            for j in range(len(args)):\n",
    "                mask_length = np.prod(mask_shapes[i][j])\n",
    "                arg_values[j].append(values[i][pos:pos+mask_length])\n",
    "                pos += mask_length\n",
    "\n",
    "        expected_values = pack_values(expected_values)\n",
    "        main_effects = pack_values(main_effects)\n",
    "        output_indices = pack_values(output_indices)\n",
    "        main_effects = pack_values(main_effects)\n",
    "        hierarchical_values = pack_values(hierarchical_values)\n",
    "        clustering = pack_values(clustering)\n",
    "\n",
    "        ragged_outputs = False\n",
    "        if output_indices is not None:\n",
    "            ragged_outputs = not all(len(x) == len(output_indices[0]) for x in output_indices)\n",
    "        if self.output_names is None:\n",
    "            if None not in output_names:\n",
    "                if not ragged_outputs:\n",
    "                    sliced_labels = np.array(output_names)\n",
    "                else:\n",
    "                    sliced_labels = [np.array(output_names[i])[index_list] for i,index_list in enumerate(output_indices)]\n",
    "            else:\n",
    "                sliced_labels = None\n",
    "        else:\n",
    "            labels = np.array(self.output_names)\n",
    "            sliced_labels = [labels[index_list] for index_list in output_indices]\n",
    "            if not ragged_outputs:\n",
    "                sliced_labels = np.array(sliced_labels)\n",
    "\n",
    "        if isinstance(sliced_labels, np.ndarray) and len(sliced_labels.shape) == 2:\n",
    "            if np.all(sliced_labels[0,:] == sliced_labels):\n",
    "                sliced_labels = sliced_labels[0]\n",
    "\n",
    "        out = []\n",
    "        for j in range(len(args)):\n",
    "\n",
    "            tmp = []\n",
    "            for i, v in enumerate(arg_values[j]):\n",
    "                if np.prod(mask_shapes[i][j]) != np.prod(v.shape):\n",
    "                    tmp.append(v.reshape(*mask_shapes[i][j], -1))\n",
    "                else:\n",
    "                    tmp.append(v.reshape(*mask_shapes[i][j]))\n",
    "            arg_values[j] = pack_values(tmp)\n",
    "\n",
    "            if hasattr(self.masker, \"data_transform\"):\n",
    "                data = pack_values([self.masker.data_transform(v) for v in args[j]])\n",
    "            else:\n",
    "                data = args[j]\n",
    "\n",
    "            out.append(Explanation(\n",
    "                arg_values[j], expected_values, data,\n",
    "                feature_names=feature_names[j], main_effects=main_effects,\n",
    "                clustering=clustering,\n",
    "                hierarchical_values=hierarchical_values,\n",
    "                output_names=sliced_labels \n",
    "            ))\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def explain_row(self, *row_args, max_evals, main_effects, error_bounds, outputs, silent, **kwargs):\n",
    "      \n",
    "        \n",
    "        return {}\n",
    "\n",
    "    @staticmethod\n",
    "    def supports_model_with_masker(model, masker):\n",
    "        \n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_main_effects(fm, expected_value, inds):\n",
    "       \n",
    "\n",
    "        # mask each input on in isolation\n",
    "        masks = np.zeros(2*len(inds)-1, dtype=np.int)\n",
    "        last_ind = -1\n",
    "        for i in range(len(inds)):\n",
    "            if i > 0:\n",
    "                masks[2*i - 1] = -last_ind - 1 # turn off the last input\n",
    "            masks[2*i] = inds[i] # turn on this input\n",
    "            last_ind = inds[i]\n",
    "\n",
    "        # compute the main effects for the given indexes\n",
    "        main_effects = fm(masks) - expected_value\n",
    "\n",
    "        # expand the vector to the full input size\n",
    "        expanded_main_effects = np.zeros(len(fm))\n",
    "        for i, ind in enumerate(inds):\n",
    "            expanded_main_effects[ind] = main_effects[i]\n",
    "\n",
    "        return expanded_main_effects\n",
    "\n",
    "    def save(self, out_file, model_saver=\".save\", masker_saver=\".save\"):\n",
    "        super().save(out_file)\n",
    "        with Serializer(out_file, \"shap.Explainer\", version=0) as s:\n",
    "            s.save(\"model\", self.model, model_saver)\n",
    "            s.save(\"masker\", self.masker, masker_saver)\n",
    "            s.save(\"link\", self.link)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, in_file, model_loader=Model.load, masker_loader=Masker.load, instantiate=True):\n",
    "      \n",
    "        if instantiate:\n",
    "            return cls._instantiated_load(in_file, model_loader=model_loader, masker_loader=masker_loader)\n",
    "\n",
    "        kwargs = super().load(in_file, instantiate=False)\n",
    "        with Deserializer(in_file, \"shap.Explainer\", min_version=0, max_version=0) as s:\n",
    "            kwargs[\"model\"] = s.load(\"model\", model_loader)\n",
    "            kwargs[\"masker\"] = s.load(\"masker\", masker_loader)\n",
    "            kwargs[\"link\"] = s.load(\"link\")\n",
    "        return kwargs\n",
    "\n",
    "def pack_values(values):\n",
    "\n",
    "    # collapse the values if we didn't compute them\n",
    "    if values is None or values[0] is None:\n",
    "        return None\n",
    "    elif np.issubdtype(type(values[0]), np.number) or len(np.unique([len(v) for v in values])) == 1:\n",
    "        return np.array(values)\n",
    "    else:\n",
    "        return np.array(values, dtype=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGzlw3-cD7b-"
   },
   "outputs": [],
   "source": [
    "class Deep(Explainer):\n",
    "    def __init__(self, model, data, session=None, learning_phase_flags=None):\n",
    "        \n",
    "        # first, we need to find the framework\n",
    "        if type(model) is tuple:\n",
    "            a, b = model\n",
    "            try:\n",
    "                a.named_parameters()\n",
    "                framework = 'pytorch'\n",
    "            except:\n",
    "                framework = 'tensorflow'\n",
    "        else:\n",
    "            try:\n",
    "                model.named_parameters()\n",
    "                framework = 'pytorch'\n",
    "            except:\n",
    "                framework = 'tensorflow'\n",
    "\n",
    "        if framework == 'tensorflow':\n",
    "            self.explainer = TFDeep(model, data, session, learning_phase_flags)\n",
    "        elif framework == 'pytorch':\n",
    "            self.explainer = PyTorchDeep(model, data)\n",
    "\n",
    "        self.expected_value = self.explainer.expected_value\n",
    "\n",
    "    def shap_values(self, X, ranked_outputs=None, output_rank_order='max', check_additivity=True):\n",
    "        \n",
    "        return self.explainer.shap_values(X, ranked_outputs, output_rank_order, check_additivity=check_additivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uKIDNFTM0NQ"
   },
   "source": [
    "## Testing Shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3QBQhBEnDM7r"
   },
   "outputs": [],
   "source": [
    "model.load_weights('/content/drive/MyDrive/DataForColob/cifar_weights_custom.h5')\n",
    "test_preds = model.predict(X_test_prep)\n",
    "test_preds_classes = np.argmax(test_preds, axis=-1)\n",
    "test_accuracy = np.mean(test_preds_classes == y_test)\n",
    "\n",
    "np.random.seed(20)\n",
    "background = X_train_prep[np.random.choice(X_train_prep.shape[0], 100, replace=False)]\n",
    "explainer = shap.DeepExplainer(model, background)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "o5RhjfJ8Dap-",
    "outputId": "86bd4013-9aa1-40bd-f80c-604fe808176f"
   },
   "outputs": [],
   "source": [
    "sample_idx = 1234\n",
    "sample_img = X_test[sample_idx]\n",
    "sample_img_prep = X_test_prep[sample_idx]\n",
    "\n",
    "sample_label = y_test[sample_idx]\n",
    "sample_pred = test_preds[sample_idx]\n",
    "\n",
    "for i in range(10):\n",
    "    rounded_pred = str(sample_pred[i].round(3))\n",
    "    print(f'{class_names[i]}:    \\t{rounded_pred}')\n",
    "    \n",
    "plt.imshow(sample_img)\n",
    "plt.title(class_names[sample_label])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5ZwdBQsDYBf",
    "outputId": "8f90999a-238c-471b-b006-b09c21ae9e21"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "batch = sample_img_prep[np.newaxis, :]\n",
    "batch_shap = explainer.shap_values(batch)\n",
    "sample_img_shap = batch_shap[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RYEXR9cD_Qo"
   },
   "source": [
    "## Analysis of a typical sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "t9t0-2SvDpG6",
    "outputId": "473beb00-a76e-4cba-9bcb-2c38113566c6"
   },
   "outputs": [],
   "source": [
    "def prep_img(img):\n",
    "    return np.array([img / 255])\n",
    "\n",
    "shap.image_plot(batch_shap, prep_img(sample_img), labels=shap_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMwlmPceEE5H"
   },
   "source": [
    "## Analysis of samples from classes that are difficult to distinguish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igQ2qlMZETwj"
   },
   "source": [
    "### Data set visualization with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "NuGDyvp5EUwf",
    "outputId": "fa3ac4b9-c4d9-4827-bf1e-f1c5bf7b6c83"
   },
   "outputs": [],
   "source": [
    "batch_size = 2_000\n",
    "batch_indices = np.random.choice(len(X_train), batch_size, replace=False)\n",
    "X_batch = X_train[batch_indices].reshape((-1, 32 * 32 * 3))\n",
    "batch_labels = y_train[batch_indices]\n",
    "\n",
    "pca = PCA(n_components=2).fit(X_batch)\n",
    "X_batch_2d = pca.transform(X_batch)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for i, name in class_names.items():\n",
    "    class_indices = batch_labels == i\n",
    "    X_class = X_batch_2d[class_indices]\n",
    "    plt.scatter(X_class[:, 0], X_class[:, 1], s=10, label=name)\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWeDfuC8EekY"
   },
   "source": [
    "### Visualization of the dataset with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "fQ2MpfRaEfhi",
    "outputId": "32382712-dc5e-4ca4-f72f-49548f4ad696"
   },
   "outputs": [],
   "source": [
    "batch_size = 2_000\n",
    "batch_indices = np.random.choice(len(X_train), batch_size, replace=False)\n",
    "X_batch = X_train[batch_indices].reshape((-1, 32 * 32 * 3))\n",
    "batch_labels = y_train[batch_indices]\n",
    "\n",
    "X_batch_2d = TSNE(n_components=2).fit_transform(X_batch)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for i, name in class_names.items():\n",
    "    class_indices = batch_labels == i\n",
    "    X_class = X_batch_2d[class_indices]\n",
    "    plt.scatter(X_class[:, 0], X_class[:, 1], s=10, label=name)\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSLfnLFbEnVs"
   },
   "source": [
    "### Confusion matrix analysis for prediction on a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "Xz7apVXzEhn2",
    "outputId": "5dc201c5-32b3-430c-9d55-1fa9287fcd9d"
   },
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, test_preds_classes, normalize='true')\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(conf_mat)\n",
    "labels = list(class_names.values())\n",
    "plt.xticks(range(10), labels=labels, rotation=90)\n",
    "plt.yticks(range(10), labels=labels)\n",
    "plt.xlabel('predicted class')\n",
    "plt.ylabel('true class')\n",
    "plt.title('confusion matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "conf_mat_masked = conf_mat.copy()\n",
    "for i in range(10):\n",
    "    conf_mat_masked[i, i] = 0\n",
    "    \n",
    "plt.imshow(conf_mat_masked)\n",
    "labels = list(class_names.values())\n",
    "plt.xticks(range(10), labels=labels, rotation=90)\n",
    "plt.yticks(range(10), labels=labels)\n",
    "plt.xlabel('predicted class')\n",
    "plt.ylabel('true class')\n",
    "plt.title('confusion matrix with removed diagonal')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWrokpn7EztP"
   },
   "source": [
    "### Shap value analysis - compare key image areas of similar classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Be3wtbFWErJX"
   },
   "outputs": [],
   "source": [
    "def compare_shap(shap_values, explained_classes, imgs):\n",
    "    np_shap = np.array(shap_values)\n",
    "    class_labels = [class_names[c] for c in explained_classes]\n",
    "    classes_shap = np_shap[explained_classes]\n",
    "    for i, img in enumerate(imgs):\n",
    "        img_shap = list(classes_shap[:, i:i+1])\n",
    "        shap.image_plot(img_shap, prep_img(img), np.array([class_labels]))\n",
    "        \n",
    "        \n",
    "def blur_shap_values(shap_values, sigma):\n",
    "    classes_count = len(shap_values)\n",
    "    samples_count = shap_values[0].shape[0]\n",
    "    \n",
    "    blurred_shap_values = []\n",
    "    for class_idx in range(classes_count):\n",
    "        blurred = []\n",
    "        for sample_idx in range(samples_count):\n",
    "            blurred.append(gaussian_filter(shap_values[class_idx][sample_idx], sigma=sigma))\n",
    "        blurred_shap_values.append(blurred)\n",
    "    return blurred_shap_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hi_pATL6E69e"
   },
   "source": [
    "### Shap values analysis: cats vs dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NR0urYJ-E2Wd",
    "outputId": "fe263ff0-c6ad-488c-8662-0e15f29139d1"
   },
   "outputs": [],
   "source": [
    "\n",
    "cat_class = 3\n",
    "dog_class = 5\n",
    "\n",
    "# ręcznie dobrane przykłady\n",
    "cat_indices = [103, 336, 432, 558, 573, 673, 874]\n",
    "dog_indices = [39, 42, 181, 190, 207, 232, 319]\n",
    "\n",
    "indices = cat_indices\n",
    "samples = X_test[indices]\n",
    "samples_prep = X_test_prep[indices]\n",
    "\n",
    "samples_shap = explainer.shap_values(samples_prep)\n",
    "compare_shap(samples_shap, [cat_class, dog_class], samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sPXqcnAqFVm0",
    "outputId": "b1f3f8c7-341f-427b-c71e-4b3a56d33940"
   },
   "outputs": [],
   "source": [
    "samples_shap_blurred = blur_shap_values(samples_shap, sigma=1)\n",
    "compare_shap(samples_shap_blurred, [cat_class, dog_class], samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Tgrf2lAfFYut",
    "outputId": "027b0d65-8f1f-48fb-bf1c-ed9dcf90a697"
   },
   "outputs": [],
   "source": [
    "indices = dog_indices\n",
    "samples = X_test[indices]\n",
    "samples_prep = X_test_prep[indices]\n",
    "\n",
    "samples_shap = explainer.shap_values(samples_prep)\n",
    "compare_shap(samples_shap, [cat_class, dog_class], samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GX78UjKnFa3K",
    "outputId": "79c28848-c85a-404a-b00a-96304fb53d9c"
   },
   "outputs": [],
   "source": [
    "samples_shap_blurred = blur_shap_values(samples_shap, sigma=1)\n",
    "compare_shap(samples_shap_blurred, [cat_class, dog_class], samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0xLCj-dFg-h"
   },
   "source": [
    "### Shap values analysis: horses vs deers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OI0-RFnxFkbC",
    "outputId": "d8a08f50-7c61-420a-c441-f04a29ce90f4"
   },
   "outputs": [],
   "source": [
    "deer_class = 4\n",
    "horse_class = 7\n",
    "\n",
    "# ręcznie dobrane przykłady\n",
    "deer_indices = [22, 36, 40, 117, 159, 227, 505]\n",
    "horse_indices = [13, 17, 83, 109, 210, 216, 506]\n",
    "indices = deer_indices\n",
    "np.random.seed(30)\n",
    "background = X_train_prep[np.random.choice(X_train_prep.shape[0], 100, replace=False)]\n",
    "explainer = shap.DeepExplainer(model, background)\n",
    "\n",
    "samples = X_test[indices]\n",
    "samples_prep = X_test_prep[indices]\n",
    "\n",
    "samples_shap = explainer.shap_values(samples_prep)\n",
    "compare_shap(samples_shap, [deer_class, horse_class], samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hBmlXG2QFrNf",
    "outputId": "3e3be03b-c705-403b-f833-a44c02e4a494"
   },
   "outputs": [],
   "source": [
    "samples_shap_blurred = blur_shap_values(samples_shap, sigma=1)\n",
    "compare_shap(samples_shap_blurred, [deer_class, horse_class], samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JhpxYaZmFuKf",
    "outputId": "879a2857-58f7-4371-b40d-0553e7ae0d96"
   },
   "outputs": [],
   "source": [
    "indices = horse_indices\n",
    "np.random.seed(30)\n",
    "\n",
    "samples = X_test[indices]\n",
    "samples_prep = X_test_prep[indices]\n",
    "\n",
    "samples_shap = explainer.shap_values(samples_prep)\n",
    "compare_shap(samples_shap, [deer_class, horse_class], samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "X1PXR93bFwu5",
    "outputId": "82c3b088-f635-4895-836f-64307522d8b6"
   },
   "outputs": [],
   "source": [
    "samples_shap_blurred = blur_shap_values(samples_shap, sigma=1)\n",
    "compare_shap(samples_shap_blurred, [deer_class, horse_class], samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxMFJKCdF5mp"
   },
   "source": [
    "### Selection of the easiest and most difficult samples to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlmqwITqF6kY"
   },
   "outputs": [],
   "source": [
    "SAMPLES_PER_CLASS = 3\n",
    "\n",
    "best_samples_indices = []\n",
    "worst_samples_indices = []\n",
    "\n",
    "for class_index in range(10):\n",
    "    class_mask = y_test.flatten() == class_index\n",
    "    class_preds = test_preds[:, class_index]\n",
    "    indices = range(len(class_preds))\n",
    "    \n",
    "    best_preds_sorted = sorted(zip(class_preds * class_mask, indices), reverse=True)\n",
    "    best_selected_indices = [i[1] for i in best_preds_sorted]\n",
    "    best_samples_indices.append(best_selected_indices[:SAMPLES_PER_CLASS])\n",
    "    \n",
    "    worst_preds_sorted = sorted(zip((1 - class_preds) * class_mask, indices), reverse=True)\n",
    "    worst_selected_indices = [i[1] for i in worst_preds_sorted]\n",
    "    worst_samples_indices.append(worst_selected_indices[:SAMPLES_PER_CLASS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "GJMDbz_LF-LG",
    "outputId": "7c9a346b-b6d8-4b50-a527-5acfa0356724"
   },
   "outputs": [],
   "source": [
    "print('Samples classified correctly with the greatest certainty')\n",
    "plt.figure(figsize=(20, 6))\n",
    "for class_index, img_indices in enumerate(best_samples_indices):\n",
    "    for sample_index, img_index in enumerate(img_indices):\n",
    "        plt.subplot(SAMPLES_PER_CLASS, 10, sample_index * 10 + class_index + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X_test[img_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VkFhKoSYF_5M",
    "outputId": "71973ffa-2c86-4f69-ba53-4e7a47a124e6"
   },
   "outputs": [],
   "source": [
    "for class_idx, samples_indices in enumerate(best_samples_indices):\n",
    "    print(f'Explaining for samples classified correctly with the highest certainty (class {class_names[class_idx]})')\n",
    "    imgs = X_test[samples_indices]\n",
    "    imgs_prep = X_test_prep[samples_indices]\n",
    "    shap_values = explainer.shap_values(imgs_prep)\n",
    "    shap_values_blurred = blur_shap_values(shap_values, sigma=1)\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        shap_values_for_img = list(np.array(shap_values_blurred)[:, i:i+1])\n",
    "        compare_shap(shap_values_for_img, list(range(10)), np.array([img]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmpAKZhRGDUi",
    "outputId": "921f28df-70bd-4c22-f9a8-8d5352099c9a"
   },
   "outputs": [],
   "source": [
    "print('Samples classified incorrectly with the greatest certainty')\n",
    "plt.figure(figsize=(20, 6))\n",
    "for class_index, img_indices in enumerate(worst_samples_indices):\n",
    "    for sample_index, img_index in enumerate(img_indices):\n",
    "        plt.subplot(SAMPLES_PER_CLASS, 10, sample_index * 10 + class_index + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X_test[img_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XzxPvCw8GFKo",
    "outputId": "26be1e9a-a940-40ec-884a-0eaf40dcc5b3"
   },
   "outputs": [],
   "source": [
    "for class_idx, samples_indices in enumerate(worst_samples_indices):\n",
    "    print(f'Explaining for samples classified incorrectly with the highest certainty (class {class_names[class_idx]})')\n",
    "    imgs = X_test[samples_indices]\n",
    "    imgs_prep = X_test_prep[samples_indices]\n",
    "    true_labels = y_test[samples_indices]\n",
    "    pred_labels = test_preds_classes[samples_indices]\n",
    "            \n",
    "    shap_values = explainer.shap_values(imgs_prep)\n",
    "    shap_values_blurred = blur_shap_values(shap_values, sigma=1)\n",
    "    \n",
    "    for i, (img, true_label, pred_label) in enumerate(zip(imgs, true_labels, pred_labels)):\n",
    "        shap_values_for_img = list(np.array(shap_values_blurred)[:, i:i+1])\n",
    "        compare_shap(shap_values_for_img, [pred_label, true_label], np.array([img]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYesLLZ5GMvf"
   },
   "source": [
    "### Selection of samples for which the model prediction is aligned for two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZAkIutdGNhE",
    "outputId": "2ffe29e6-ba60-4b95-bb72-913ecb9b442e"
   },
   "outputs": [],
   "source": [
    "CONFUSING_SAMPLES_COUNT = 10\n",
    "CONFIDENCE_THRESHOLD = 0.4\n",
    "\n",
    "preds_above_threshold = test_preds > CONFIDENCE_THRESHOLD\n",
    "confusing_preds_mask = np.sum(preds_above_threshold, axis=-1) > 1\n",
    "confusing_samples_indices = np.argwhere(confusing_preds_mask)[:CONFUSING_SAMPLES_COUNT].flatten()\n",
    "confusing_samples_preds = [np.argwhere(preds_above_threshold[i]).flatten() for i in confusing_samples_indices]\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "for i, (sample_index, sample_preds) in enumerate(zip(confusing_samples_indices, confusing_samples_preds)):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_test[sample_index])\n",
    "    true_label = class_names[y_test[sample_index]]\n",
    "    plt.title(f'{class_names[sample_preds[0]]} / {class_names[sample_preds[1]]}, true={true_label}')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "w1XB0ceUGSYl",
    "outputId": "6ea23de6-fcf0-4257-cf57-b3c2c8842420"
   },
   "outputs": [],
   "source": [
    "for sample_index, predicted_classes in zip(confusing_samples_indices, confusing_samples_preds):\n",
    "    img = X_test[sample_index]\n",
    "    img_prep = X_test_prep[sample_index]\n",
    "    true_label = y_test[sample_index]\n",
    "    print(f'Predicted classes {class_names[predicted_classes[0]]}, {class_names[predicted_classes[1]]}')\n",
    "    print(f'Actual class {class_names[true_label]}')\n",
    "        \n",
    "    shap_values = explainer.shap_values(np.array([img_prep]))\n",
    "    shap_values_blurred = blur_shap_values(shap_values, sigma=1)\n",
    "    explained_classes = np.unique(list(predicted_classes) + [true_label])\n",
    "    compare_shap(shap_values_for_img, explained_classes, np.array([img]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oqs3XV6nGXYD"
   },
   "source": [
    "### Analysis of the received explanations for various backgrouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "n-qRVVpVGYX6",
    "outputId": "5d9a9c9f-84a9-4ecf-8efc-1ef326f30226"
   },
   "outputs": [],
   "source": [
    "sample_idx = 4\n",
    "sample_img = X_test[sample_idx]\n",
    "sample_img_prep = X_test_prep[sample_idx]\n",
    "\n",
    "sample_label = y_test[sample_idx]\n",
    "sample_pred = test_preds[sample_idx]\n",
    "\n",
    "for i in range(10):\n",
    "    rounded_pred = str(sample_pred[i].round(3))\n",
    "    print(f'{class_names[i]}:    \\t{rounded_pred}')\n",
    "    \n",
    "plt.imshow(sample_img)\n",
    "plt.title(class_names[sample_label])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yX3y7GxbGbl2",
    "outputId": "a25098f0-662c-4674-d73b-a90423864d4e"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "np.random.seed(20)\n",
    "\n",
    "backgrounds = {\n",
    "    '10 random samples': X_train_prep[np.random.choice(X_train_prep.shape[0], 10, replace=False)],\n",
    "    '100 random samples': X_train_prep[np.random.choice(X_train_prep.shape[0], 100, replace=False)],\n",
    "    '1000 random samples': X_train_prep[np.random.choice(X_train_prep.shape[0], 1000, replace=False)],\n",
    "    'Blank black image': np.zeros((1, *X_train_prep[0].shape)),\n",
    "    'Blank white image': np.ones((1, *X_train_prep[0].shape)),\n",
    "    'Mean of training set': np.mean(X_train_prep, axis=0, keepdims=True),\n",
    "    'Median of training set': np.median(X_train_prep, axis=0, keepdims=True),\n",
    "    'Geometric mean of training set': preprocess_input(gmean(X_train, axis=0)[np.newaxis, :]),\n",
    "    'Harmonic mean of training set': preprocess_input(hmean(X_train, axis=0)[np.newaxis, :]),\n",
    "    '100 samples of the same class': X_train_prep[np.random.choice(np.argwhere(y_train==sample_label).flatten(), 100, replace=False)],\n",
    "    '100 samples of wrong classes': X_train_prep[np.random.choice(np.argwhere(y_train!=sample_label).flatten(), 100, replace=False)],\n",
    "    '100 samples of single wrong class (ships)': X_train_prep[np.random.choice(np.argwhere(y_train==8).flatten(), 100, replace=False)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "r8_2p0uNGdw4",
    "outputId": "72e5e97d-31b0-415c-9a1a-b8d279a05aa4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "image_batch = sample_img_prep[np.newaxis, :]\n",
    "\n",
    "for background_name, background in backgrounds.items():\n",
    "    print(f'{background_name} background:')\n",
    "    explainer = shap.DeepExplainer(model, background)\n",
    "    sample_img_shap = explainer.shap_values(image_batch)\n",
    "    compare_shap(sample_img_shap, list(range(10)), [sample_img])\n",
    "    compare_shap(blur_shap_values(sample_img_shap, 1), list(range(10)), [sample_img])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CIFAR10_kernelshap.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
