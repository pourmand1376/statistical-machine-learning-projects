{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7-OUcUrGlts"
   },
   "outputs": [],
   "source": [
    "## Amir Pourmand\n",
    "## This is done! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPP2IxnkQmTE"
   },
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803,
     "referenced_widgets": [
      "d2dcfbcba04b46ddb9984361dd86fd2c",
      "4a6bb86811a747839816b949447b4565",
      "5304c1dbb8fd4b01b48cb123605007a4",
      "9cc6522b757646cfb64c73f033e88c12",
      "0fc287afa5384d7aa464619883cc08f1",
      "e0e12e9ad0554075a10c72a7dd7afb17",
      "7b9fa0e50d8d4706bf727f39ba231fa0",
      "fc14d94f56554bb6bdf9aa3d465abc84",
      "ba7111f863aa4ee58d778817bc2529f1",
      "65c09095efba45e59e624108a61ae109",
      "36e700da083b46309ce215f187afbf7f",
      "24a56742a9044889b9cdb7a42fe264dc",
      "e1228a1cb76f44c18fee5cdfad29054d",
      "fd59ca81351e4b3e82dd06e06d0349ba",
      "a55209163302433b92a96cf1f2210f6d",
      "10b441d4c27f467993b67fdca75dd399",
      "aee4a61cdbb141ee895838a410fc48ea",
      "8d1f7b58788542fb9620238c68601c74",
      "2f65be33b22047ba8a1fbe0229834852",
      "a4bfa0f9274a4cf6a26ef72f8573e4a0",
      "added4c7e1b14cd78fa3b6edb050fc03",
      "93eb8b24373a4ef59b39e288c85a085f",
      "32b515b77c064a6681baabf531b05e34",
      "6af8cc8d8b404730a0f9f43f22cdef71",
      "10c89159f8414dd2a036473713abb5b0",
      "1eb65b30582a4f6d91b7d21e73783654",
      "333b2f46a376423a83013b5ecd6cb304",
      "46e7b7a9f7b142c3a88fa5a3a4e35058",
      "5153c3d3005b4990802e72c1b1dc7d5e",
      "ed825f77af394645a35a35b99fedf81e",
      "a6d0dfa85f1d4df9abcab9e2f9a9297e",
      "837c1cd7823342baa7d2b8a0085109b4"
     ]
    },
    "id": "Fls88AFjQ1O0",
    "outputId": "f02bba78-fd21-4de8-8241-9da02505ac42"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 2\n",
    "device = torch.device('cpu')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 20, kernel_size=5),\n",
    "            nn.Dropout(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(320, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(-1, 320)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output.log(), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output.log(), target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('mnist_data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor()\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('mnist_data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor()\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjJ9U7M3FRYV"
   },
   "source": [
    "## Shap Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIibcXTxFP2s"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import inspect\n",
    "import logging\n",
    "import numpy as np\n",
    "import cloudpickle\n",
    "\n",
    "log = logging.getLogger('shap')\n",
    "\n",
    "class Serializable():\n",
    "\n",
    "    def save(self, out_file):\n",
    "        pickle.dump(type(self), out_file)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, in_file, instantiate=True):\n",
    "        if instantiate:\n",
    "            return cls._instantiated_load(in_file)\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def _instantiated_load(cls, in_file, **kwargs):\n",
    "        obj_type = pickle.load(in_file)\n",
    "        if obj_type is None:\n",
    "            return None\n",
    "\n",
    "        if not inspect.isclass(obj_type) or (not issubclass(obj_type, cls) and (obj_type is not cls)):\n",
    "            raise Exception(f\"Invalid object type loaded from file. {obj_type} is not a subclass of {cls}.\")\n",
    "\n",
    "        # here we call the constructor with all the arguments we have loaded\n",
    "        constructor_args = obj_type.load(in_file, instantiate=False, **kwargs)\n",
    "        used_args = inspect.getfullargspec(obj_type.__init__)[0]\n",
    "        return obj_type(**{k: constructor_args[k] for k in constructor_args if k in used_args})\n",
    "\n",
    "\n",
    "class Serializer():\n",
    "    def __init__(self, out_stream, block_name, version):\n",
    "        self.out_stream = out_stream\n",
    "        self.block_name = block_name\n",
    "        self.block_version = version\n",
    "        self.serializer_version = 0 # update this when the serializer changes\n",
    "\n",
    "    def __enter__(self):\n",
    "        log.debug(\"serializer_version = %d\", self.serializer_version)\n",
    "        pickle.dump(self.serializer_version, self.out_stream)\n",
    "        log.debug(\"block_name = %s\", self.block_name)\n",
    "        pickle.dump(self.block_name, self.out_stream)\n",
    "        log.debug(\"block_version = %d\", self.block_version)\n",
    "        pickle.dump(self.block_version, self.out_stream)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        log.debug(\"END_BLOCK___\")\n",
    "        pickle.dump(\"END_BLOCK___\", self.out_stream)\n",
    "\n",
    "    def save(self, name, value, encoder=\"auto\"):\n",
    "        log.debug(\"name = %s\", name)\n",
    "        pickle.dump(name, self.out_stream)\n",
    "        if encoder is None or encoder is False:\n",
    "            log.debug(\"encoder_name = %s\", \"no_encoder\")\n",
    "            pickle.dump(\"no_encoder\", self.out_stream)\n",
    "        elif callable(encoder):\n",
    "            log.debug(\"encoder_name = %s\", \"custom_encoder\")\n",
    "            pickle.dump(\"custom_encoder\", self.out_stream)\n",
    "            encoder(value, self.out_stream)\n",
    "        elif encoder == \".save\" or (isinstance(value, Serializable) and encoder == \"auto\"):\n",
    "            log.debug(\"encoder_name = %s\", \"serializable.save\")\n",
    "            pickle.dump(\"serializable.save\", self.out_stream)\n",
    "            if len(inspect.getfullargspec(value.save)[0]) == 3: # backward compat for MLflow, can remove 4/1/2021\n",
    "                value.save(self.out_stream, value)\n",
    "            else:\n",
    "                value.save(self.out_stream)\n",
    "        elif encoder == \"auto\":\n",
    "            if isinstance(value, (int, float, str)):\n",
    "                log.debug(\"encoder_name = %s\", \"pickle.dump\")\n",
    "                pickle.dump(\"pickle.dump\", self.out_stream)\n",
    "                pickle.dump(value, self.out_stream)\n",
    "            else:\n",
    "                log.debug(\"encoder_name = %s\", \"cloudpickle.dump\")\n",
    "                pickle.dump(\"cloudpickle.dump\", self.out_stream)\n",
    "                cloudpickle.dump(value, self.out_stream)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown encoder type '{encoder}' given for serialization!\")\n",
    "        log.debug(\"value = %s\", str(value))\n",
    "\n",
    "class Deserializer():\n",
    "    \"\"\" Load data items from an input stream.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_stream, block_name, min_version, max_version):\n",
    "        self.in_stream = in_stream\n",
    "        self.block_name = block_name\n",
    "        self.block_min_version = min_version\n",
    "        self.block_max_version = max_version\n",
    "\n",
    "        # update these when the serializer changes\n",
    "        self.serializer_min_version = 0\n",
    "        self.serializer_max_version = 0\n",
    "\n",
    "    def __enter__(self):\n",
    "\n",
    "        # confirm the serializer version\n",
    "        serializer_version = pickle.load(self.in_stream)\n",
    "        log.debug(\"serializer_version = %d\", serializer_version)\n",
    "        if serializer_version < self.serializer_min_version:\n",
    "            raise ValueError(\n",
    "                f\"The file being loaded was saved with a serializer version of {serializer_version}, \" + \\\n",
    "                f\"but the current deserializer in SHAP requires at least version {self.serializer_min_version}.\"\n",
    "            )\n",
    "        if serializer_version > self.serializer_max_version:\n",
    "            raise ValueError(\n",
    "                f\"The file being loaded was saved with a serializer version of {serializer_version}, \" + \\\n",
    "                f\"but the current deserializer in SHAP only support up to version {self.serializer_max_version}.\"\n",
    "            )\n",
    "\n",
    "        # confirm the block name\n",
    "        block_name = pickle.load(self.in_stream)\n",
    "        log.debug(\"block_name = %s\", block_name)\n",
    "        if block_name != self.block_name:\n",
    "            raise ValueError(\n",
    "                f\"The next data block in the file being loaded was supposed to be {self.block_name}, \" + \\\n",
    "                f\"but the next block found was {block_name}.\"\n",
    "            )\n",
    "\n",
    "        # confirm the block version\n",
    "        block_version = pickle.load(self.in_stream)\n",
    "        log.debug(\"block_version = %d\", block_version)\n",
    "        if block_version < self.block_min_version:\n",
    "            raise ValueError(\n",
    "                f\"The file being loaded was saved with a block version of {block_version}, \" + \\\n",
    "                f\"but the current deserializer in SHAP requires at least version {self.block_min_version}.\"\n",
    "            )\n",
    "        if block_version > self.block_max_version:\n",
    "            raise ValueError(\n",
    "                f\"The file being loaded was saved with a block version of {block_version}, \" + \\\n",
    "                f\"but the current deserializer in SHAP only support up to version {self.block_max_version}.\"\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        # confirm the block end token\n",
    "        for _ in range(100):\n",
    "            end_token = pickle.load(self.in_stream)\n",
    "            log.debug(\"end_token = %s\", end_token)\n",
    "            if end_token == \"END_BLOCK___\":\n",
    "                return\n",
    "            else:\n",
    "                self._load_data_value()\n",
    "        raise ValueError(\n",
    "            f\"The data block end token wsa not found for the block {self.block_name}.\"\n",
    "        )\n",
    "\n",
    "    def load(self, name, decoder=None):\n",
    "        \"\"\" Load a data item from the current input stream.\n",
    "        \"\"\"\n",
    "        # confirm the block name\n",
    "        loaded_name = pickle.load(self.in_stream)\n",
    "        log.debug(\"loaded_name = %s\", loaded_name)\n",
    "        print(\"loaded_name\", loaded_name)\n",
    "        if loaded_name != name:\n",
    "            raise ValueError(\n",
    "                f\"The next data item in the file being loaded was supposed to be {name}, \" + \\\n",
    "                f\"but the next block found was {loaded_name}.\"\n",
    "            ) # We should eventually add support for skipping over unused data items in old formats...\n",
    "\n",
    "        value = self._load_data_value(decoder)\n",
    "        log.debug(\"value = %s\", str(value))\n",
    "        return value\n",
    "\n",
    "    def _load_data_value(self, decoder=None):\n",
    "        encoder_name = pickle.load(self.in_stream)\n",
    "        log.debug(\"encoder_name = %s\", encoder_name)\n",
    "        if encoder_name == \"custom_encoder\" or callable(decoder):\n",
    "            assert callable(decoder), \"You must provide a callable custom decoder for the data item {name}!\"\n",
    "            return decoder(self.in_stream)\n",
    "        if encoder_name == \"no_encoder\":\n",
    "            return None\n",
    "        if encoder_name == \"serializable.save\":\n",
    "            return Serializable.load(self.in_stream)\n",
    "        if encoder_name == \"numpy.save\":\n",
    "            return np.load(self.in_stream)\n",
    "        if encoder_name == \"pickle.dump\":\n",
    "            return pickle.load(self.in_stream)\n",
    "        if encoder_name == \"cloudpickle.dump\":\n",
    "            return cloudpickle.load(self.in_stream)\n",
    "\n",
    "        raise ValueError(f\"Unsupported encoder type found: {encoder_name}\")\n",
    "{\"mode\":\"full\",\"isActive\":False}\n",
    "\n",
    "class Masker(Serializable):\n",
    "\n",
    "    def __call__(self, mask, *args):\n",
    "        '''super class of all maskers'''\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNHDXEtyFgIN"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numba\n",
    "\n",
    "@numba.jit\n",
    "def identity(x):\n",
    "    return x\n",
    "@numba.jit\n",
    "def _identity_inverse(x):\n",
    "    return x\n",
    "identity.inverse = _identity_inverse\n",
    "\n",
    "@numba.jit\n",
    "def logit(x):\n",
    "    return np.log(x/(1-x))\n",
    "@numba.jit\n",
    "def _logit_inverse(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "logit.inverse = _logit_inverse\n",
    "\n",
    "def safe_isinstance(obj, class_path_str):\n",
    "    if isinstance(class_path_str, str):\n",
    "        class_path_strs = [class_path_str]\n",
    "    elif isinstance(class_path_str, list) or isinstance(class_path_str, tuple):\n",
    "        class_path_strs = class_path_str\n",
    "    else:\n",
    "        class_path_strs = ['']\n",
    "    \n",
    "    # try each module path in order\n",
    "    for class_path_str in class_path_strs:\n",
    "        if \".\" not in class_path_str:\n",
    "            raise ValueError(\"class_path_str must be a string or list of strings specifying a full \\\n",
    "                module path to a class. Eg, 'sklearn.ensemble.RandomForestRegressor'\")\n",
    "\n",
    "        module_name, class_name = class_path_str.rsplit(\".\", 1)\n",
    "\n",
    "        if module_name not in sys.modules:\n",
    "            continue\n",
    "\n",
    "        module = sys.modules[module_name]\n",
    "        \n",
    "        #Get class\n",
    "        _class = getattr(module, class_name, None)\n",
    "        \n",
    "        if _class is None:\n",
    "            continue\n",
    "        \n",
    "        if isinstance(obj, _class):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class ShowProgress():\n",
    "    def __init__(self, iterable, total, desc, silent, start_delay):\n",
    "        self.iter = iter(iterable)\n",
    "        self.start_time = time.time()\n",
    "        self.pbar = None\n",
    "        self.total = total\n",
    "        self.desc = desc\n",
    "        self.start_delay = start_delay\n",
    "        self.silent = silent\n",
    "        self.unshown_count = 0\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.pbar is None and time.time() - self.start_time > self.start_delay:\n",
    "           self.pbar = tqdm.tqdm(total=self.total, initial=self.unshown_count, desc=self.desc, disable=self.silent)\n",
    "           self.pbar.start_t = self.start_time\n",
    "        if self.pbar is not None:\n",
    "            self.pbar.update(1)\n",
    "        else:\n",
    "            self.unshown_count += 1\n",
    "        try:\n",
    "            return next(self.iter)\n",
    "        except StopIteration as e:\n",
    "            if self.pbar is not None:\n",
    "                self.pbar.close()\n",
    "            raise e\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "def show_progress(iterable, total=None, desc=None, silent=False, start_delay=10):\n",
    "    return ShowProgress(iterable, total, desc, silent, start_delay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YokCryP2FiwV"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Model(Serializable):\n",
    "\n",
    "    def __init__(self, model=None):\n",
    "        \"\"\" Wrap a callable model as a SHAP Model object.\n",
    "        \"\"\"\n",
    "        if isinstance(model, Model):\n",
    "            self.inner_model = model.inner_model\n",
    "        else:\n",
    "            self.inner_model = model\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        return self.inner_model(*args)\n",
    "\n",
    "    def save(self, out_file):\n",
    "        \"\"\" Save the model to the given file stream.\n",
    "        \"\"\"\n",
    "        super().save(out_file)\n",
    "        with Serializer(out_file, \"shap.Model\", version=0) as s:\n",
    "            s.save(\"model\", self.inner_model)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, in_file, instantiate=True):\n",
    "        if instantiate:\n",
    "            return cls._instantiated_load(in_file)\n",
    "\n",
    "        kwargs = super().load(in_file, instantiate=False)\n",
    "        with Deserializer(in_file, \"shap.Model\", min_version=0, max_version=0) as s:\n",
    "            kwargs[\"model\"] = s.load(\"model\")\n",
    "        return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTx_0WCFFlIL"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TeacherForcing(Model):\n",
    "\n",
    "    def __init__(self, model, tokenizer=None, similarity_model=None, similarity_tokenizer=None, batch_size=128, device=None):\n",
    "        super().__init__(model)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        # set pad token if not defined\n",
    "        if self.tokenizer is not None and self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        # assign text generation function\n",
    "        if safe_isinstance(model, \"transformers.PreTrainedModel\") or safe_isinstance(model, \"transformers.TFPreTrainedModel\"):\n",
    "            self.text_generate = models.TextGeneration(self.inner_model, tokenizer=self.tokenizer, device=self.device)\n",
    "            self.similarity_model = model\n",
    "            self.similarity_tokenizer = tokenizer\n",
    "            self.model_agnostic = False\n",
    "        else:\n",
    "            self.text_generate = models.TextGeneration(self.inner_model, device=self.device)\n",
    "            self.similarity_model = similarity_model\n",
    "            self.similarity_tokenizer = similarity_tokenizer\n",
    "            # set pad token for a similarity tokenizer(in a model agnostic scenario) if not defined\n",
    "            if self.similarity_tokenizer is not None and self.similarity_tokenizer.pad_token is None:\n",
    "                self.similarity_tokenizer.pad_token = self.similarity_tokenizer.eos_token\n",
    "            self.model_agnostic = True\n",
    "        # initializing target which is the target sentence/ids for every new row of explanation\n",
    "        self.output = None\n",
    "        self.output_names = None\n",
    "\n",
    "        self.similarity_model_type = None\n",
    "        if safe_isinstance(self.similarity_model, \"transformers.PreTrainedModel\"):\n",
    "            self.similarity_model_type = \"pt\"\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if self.device is None else self.device\n",
    "            self.similarity_model = self.similarity_model.to(self.device)\n",
    "        elif safe_isinstance(self.similarity_model, \"transformers.TFPreTrainedModel\"):\n",
    "            self.similarity_model_type = \"tf\"\n",
    "\n",
    "    def __call__(self, X, Y):\n",
    "        output_batch = None\n",
    "        # caching updates output names and target sentence ids\n",
    "        self.update_output_names(Y[:1])\n",
    "        start_batch_idx, end_batch_idx = 0, len(X)\n",
    "        while start_batch_idx < end_batch_idx:\n",
    "            X_batch = X[start_batch_idx:start_batch_idx+self.batch_size]\n",
    "            Y_batch = Y[start_batch_idx:start_batch_idx+self.batch_size]\n",
    "            logits = self.get_teacher_forced_logits(X_batch, Y_batch)\n",
    "            logodds = self.get_logodds(logits)\n",
    "            if output_batch is None:\n",
    "                output_batch = logodds\n",
    "            else:\n",
    "                output_batch = np.concatenate((output_batch, logodds))\n",
    "            start_batch_idx += self.batch_size\n",
    "        return output_batch\n",
    "\n",
    "    def update_output_names(self, output):\n",
    "        # check if the target sentence has been updated (occurs when explaining a new row)\n",
    "        if (self.output is None) or (not np.array_equal(self.output, output)):\n",
    "            self.output = output\n",
    "            self.output_names = self.get_output_names(output)\n",
    "\n",
    "    def get_output_names(self, output):\n",
    "        output_ids = self.get_outputs(output)\n",
    "        output_names = [self.similarity_tokenizer.decode([x]).strip() for x in output_ids[0, :]]\n",
    "        return output_names\n",
    "\n",
    "    def get_outputs(self, X):\n",
    "        # check if output is a sentence or already parsed target ids\n",
    "        if X.dtype.type is np.str_:\n",
    "            parsed_tokenizer_dict = parse_prefix_suffix_for_tokenizer(self.similarity_tokenizer)\n",
    "            keep_prefix, keep_suffix = parsed_tokenizer_dict['keep_prefix'], parsed_tokenizer_dict['keep_suffix']\n",
    "            if keep_suffix > 0:\n",
    "                output_ids = np.array(self.similarity_tokenizer(X.tolist(), padding=True)[\"input_ids\"])[:, keep_prefix:-keep_suffix]\n",
    "            else:\n",
    "                output_ids = np.array(self.similarity_tokenizer(X.tolist(), padding=True)[\"input_ids\"])[:, keep_prefix:]\n",
    "        else:\n",
    "            output_ids = X\n",
    "        return output_ids\n",
    "\n",
    "    def get_inputs(self, X, padding_side='right'):\n",
    "        if self.model_agnostic:\n",
    "            input_sentences = np.array(self.inner_model(X))\n",
    "        else:\n",
    "            input_sentences = np.array(X)\n",
    "        self.similarity_tokenizer.padding_side = padding_side\n",
    "        inputs = self.similarity_tokenizer(input_sentences.tolist(), return_tensors=self.similarity_model_type, padding=True)\n",
    "        self.similarity_tokenizer.padding_side = 'right'\n",
    "        return inputs\n",
    "\n",
    "    def get_logodds(self, logits):\n",
    "        if self.output.dtype.type is np.str_:\n",
    "            output_ids = self.get_outputs(self.output)[0]\n",
    "        else:\n",
    "            output_ids = self.output[0]\n",
    "\n",
    "        def calc_logodds(arr):\n",
    "            probs = np.exp(arr) / np.exp(arr).sum(-1)\n",
    "            logodds = sp.special.logit(probs)\n",
    "            return logodds\n",
    "\n",
    "        logodds = np.apply_along_axis(calc_logodds, -1, logits)\n",
    "        logodds_for_output_ids = logodds[:, np.array(range(logodds.shape[1])), output_ids]\n",
    "        return logodds_for_output_ids\n",
    "\n",
    "    def model_inference(self, inputs, output_ids):\n",
    "        if self.similarity_model_type == \"pt\":\n",
    "            inputs = inputs.to(self.device)\n",
    "            output_ids = torch.tensor(output_ids, dtype=torch.int64, device=self.device)\n",
    "            self.similarity_model.eval()\n",
    "            with torch.no_grad():\n",
    "                if self.similarity_model.config.is_encoder_decoder:\n",
    "                    outputs = self.similarity_model(**inputs, decoder_input_ids=output_ids, labels=output_ids, return_dict=True)\n",
    "                else:\n",
    "                    inputs[\"input_ids\"] = torch.cat((inputs[\"input_ids\"], output_ids), dim=-1)\n",
    "                    attention_mask_for_output_ids = torch.ones(output_ids.shape, dtype=output_ids.dtype, device=self.device)\n",
    "                    inputs[\"attention_mask\"] = torch.cat((inputs[\"attention_mask\"], attention_mask_for_output_ids), dim=-1)\n",
    "                    inputs[\"position_ids\"] = (inputs[\"attention_mask\"].long().cumsum(-1) - 1)\n",
    "                    inputs[\"position_ids\"].masked_fill_(inputs[\"attention_mask\"] == 0, 0)\n",
    "                    outputs = self.similarity_model(**inputs, return_dict=True)\n",
    "                logits = outputs.logits.detach().cpu().numpy().astype('float64')\n",
    "        elif self.similarity_model_type == \"tf\":\n",
    "            output_ids = tf.convert_to_tensor(output_ids, dtype=tf.int32)\n",
    "            if self.similarity_model.config.is_encoder_decoder:\n",
    "                if self.device is None:\n",
    "                    outputs = self.similarity_model(inputs, decoder_input_ids=output_ids, labels=output_ids, return_dict=True)\n",
    "                else:\n",
    "                    try:\n",
    "                        with tf.device(self.device):\n",
    "                            outputs = self.similarity_model(inputs, decoder_input_ids=output_ids, labels=output_ids, return_dict=True)\n",
    "                    except RuntimeError as e:\n",
    "                        print(e)\n",
    "            else:\n",
    "                inputs[\"input_ids\"] = tf.concat((inputs[\"input_ids\"], output_ids), axis=-1)\n",
    "                attention_mask_for_output_ids = tf.ones(output_ids.shape, dtype=output_ids.dtype)\n",
    "                inputs[\"attention_mask\"] = tf.concat((inputs[\"attention_mask\"], attention_mask_for_output_ids), axis=-1)\n",
    "                inputs[\"position_ids\"] = tf.math.cumsum(inputs[\"attention_mask\"], axis=-1) - 1\n",
    "                inputs[\"position_ids\"] = tf.where(inputs[\"attention_mask\"] == 0, 0, inputs[\"position_ids\"])\n",
    "                if self.device is None:\n",
    "                    outputs = self.similarity_model(inputs, return_dict=True)\n",
    "                else:\n",
    "                    try:\n",
    "                        with tf.device(self.device):\n",
    "                            outputs = self.similarity_model(inputs, return_dict=True)\n",
    "                    except RuntimeError as e:\n",
    "                        print(e)\n",
    "            logits = outputs.logits.numpy().astype('float64')\n",
    "        return logits\n",
    "\n",
    "    def get_teacher_forced_logits(self, X, Y):\n",
    "        if (hasattr(self.similarity_model.config, \"is_encoder_decoder\") and not self.similarity_model.config.is_encoder_decoder) \\\n",
    "            and (hasattr(self.similarity_model.config, \"is_decoder\") and not self.similarity_model.config.is_decoder):\n",
    "            raise ValueError(\n",
    "                \"Please assign either of is_encoder_decoder or is_decoder to True in model config for extracting target sentence ids\"\n",
    "            )\n",
    "        output_ids = self.get_outputs(Y)\n",
    "        if self.similarity_model.config.is_encoder_decoder:\n",
    "            inputs = self.get_inputs(X, padding_side='right')\n",
    "            decoder_start_token_id = None\n",
    "            if hasattr(self.similarity_model.config, \"decoder_start_token_id\") and \\\n",
    "                    self.similarity_model.config.decoder_start_token_id is not None:\n",
    "                decoder_start_token_id = self.similarity_model.config.decoder_start_token_id\n",
    "            elif hasattr(self.similarity_model.config, \"bos_token_id\") and self.similarity_model.config.bos_token_id is not None:\n",
    "                decoder_start_token_id = self.similarity_model.config.bos_token_id\n",
    "            elif (hasattr(self.similarity_model.config, \"decoder\") and hasattr(self.similarity_model.config.decoder, \"bos_token_id\") and \\\n",
    "                    self.similarity_model.config.decoder.bos_token_id is not None):\n",
    "                decoder_start_token_id = self.similarity_model.config.decoder.bos_token_id\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"No decoder_start_token_id or bos_token_id defined in config for encoder-decoder generation\"\n",
    "                )\n",
    "            output_start_id = np.ones((output_ids.shape[0], 1)) * decoder_start_token_id\n",
    "            output_ids = np.concatenate((output_start_id, output_ids), axis=-1)\n",
    "            logits = self.model_inference(inputs, output_ids)\n",
    "            logits = logits[:, :-1, :]\n",
    "        else:\n",
    "            inputs = self.get_inputs(X, padding_side='left')\n",
    "            logits = self.model_inference(inputs, output_ids)\n",
    "            logits = logits[:, -output_ids.shape[1]-1:-1, :]\n",
    "        return logits\n",
    "\n",
    "    def save(self, out_file):\n",
    "        super().save(out_file)\n",
    "\n",
    "        with Serializer(out_file, \"shap.models.TeacherForcing\", version=0) as s:\n",
    "            s.save(\"tokenizer\", self.tokenizer)\n",
    "            s.save(\"similarity_model\", self.similarity_model)\n",
    "            s.save(\"similarity_tokenizer\", self.similarity_tokenizer)\n",
    "            s.save(\"batch_size\", self.batch_size)\n",
    "            s.save(\"device\", self.device)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, in_file, instantiate=True):\n",
    "        if instantiate:\n",
    "            return cls._instantiated_load(in_file)\n",
    "\n",
    "        kwargs = super().load(in_file, instantiate=False)\n",
    "        with Deserializer(in_file, \"shap.models.TeacherForcing\", min_version=0, max_version=0) as s:\n",
    "            kwargs[\"tokenizer\"] = s.load(\"tokenizer\")\n",
    "            kwargs[\"similarity_model\"] = s.load(\"similarity_model\")\n",
    "            kwargs[\"similarity_tokenizer\"] = s.load(\"similarity_tokenizer\")\n",
    "            kwargs[\"batch_size\"] = s.load(\"batch_size\")\n",
    "            kwargs[\"device\"] = s.load(\"device\")\n",
    "        return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ig6bMreFnP0"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "class Explainer(Serializable):\n",
    "\n",
    "    def __init__(self, model, masker=None, link=None, algorithm=\"auto\", output_names=None, feature_names=None, **kwargs):\n",
    "        self.model = model\n",
    "        self.output_names = output_names\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "        # wrap the incoming masker object as a shap.Masker object\n",
    "        if safe_isinstance(masker, \"pandas.core.frame.DataFrame\") or \\\n",
    "                ((safe_isinstance(masker, \"numpy.ndarray\") or sp.sparse.issparse(masker)) and len(masker.shape) == 2):\n",
    "            if algorithm == \"partition\":\n",
    "                self.masker = maskers.Partition(masker)\n",
    "            else:\n",
    "                self.masker = maskers.Independent(masker)\n",
    "        elif safe_isinstance(masker, [\"transformers.PreTrainedTokenizer\", \"transformers.tokenization_utils_base.PreTrainedTokenizerBase\"]):\n",
    "            if (safe_isinstance(self.model, \"transformers.PreTrainedModel\") or safe_isinstance(self.model, \"transformers.TFPreTrainedModel\")) and \\\n",
    "                    safe_isinstance(self.model, MODELS_FOR_SEQ_TO_SEQ_CAUSAL_LM + MODELS_FOR_CAUSAL_LM):\n",
    "                self.masker = maskers.Text(masker, mask_token=\"...\", collapse_mask_token=True)\n",
    "            else:\n",
    "                self.masker = maskers.Text(masker)\n",
    "        elif (masker is list or masker is tuple) and masker[0] is not str:\n",
    "            self.masker = maskers.Composite(*masker)\n",
    "        elif (masker is dict) and (\"mean\" in masker):\n",
    "            self.masker = maskers.Independent(masker)\n",
    "        elif masker is None and isinstance(self.model, models.TransformersPipeline):\n",
    "            return self.__init__( # pylint: disable=non-parent-init-called\n",
    "                self.model, self.model.inner_model.tokenizer,\n",
    "                link=link, algorithm=algorithm, output_names=output_names, feature_names=feature_names, **kwargs\n",
    "            )\n",
    "        else:\n",
    "            self.masker = masker\n",
    "\n",
    "        if safe_isinstance(self.model, \"transformers.pipelines.Pipeline\"):\n",
    "            return self.__init__( \n",
    "                models.TransformersPipeline(self.model), self.masker,\n",
    "                link=link, algorithm=algorithm, output_names=output_names, feature_names=feature_names, **kwargs\n",
    "            )\n",
    "\n",
    "        if (safe_isinstance(self.model, \"transformers.PreTrainedModel\") or safe_isinstance(self.model, \"transformers.TFPreTrainedModel\")) and \\\n",
    "                safe_isinstance(self.model, MODELS_FOR_SEQ_TO_SEQ_CAUSAL_LM + MODELS_FOR_CAUSAL_LM):\n",
    "            self.model = models.TeacherForcing(self.model, self.masker.tokenizer)\n",
    "            self.masker = maskers.OutputComposite(self.masker, self.model.text_generate)\n",
    "        elif safe_isinstance(self.model, \"shap.models.TeacherForcing\") and safe_isinstance(self.masker, [\"shap.maskers.Text\", \"shap.maskers.Image\"]):\n",
    "            self.masker = maskers.OutputComposite(self.masker, self.model.text_generate)\n",
    "        elif safe_isinstance(self.model, \"shap.models.TopKLM\") and safe_isinstance(self.masker, \"shap.maskers.Text\"):\n",
    "            self.masker = maskers.FixedComposite(self.masker)\n",
    "\n",
    "        if callable(link) and callable(getattr(link, \"inverse\", None)):\n",
    "            self.link = link\n",
    "        else:\n",
    "            raise Exception(\"The passed link function needs to be callable and have a callable .inverse property!\")\n",
    "\n",
    "        if self.__class__ is Explainer:\n",
    "\n",
    "            if algorithm == \"auto\":\n",
    "\n",
    "                if explainers.Linear.supports_model_with_masker(model, self.masker):\n",
    "                    algorithm = \"linear\"\n",
    "                elif explainers.Tree.supports_model_with_masker(model, self.masker): \n",
    "                    algorithm = \"tree\"\n",
    "                elif explainers.Additive.supports_model_with_masker(model, self.masker):\n",
    "                    algorithm = \"additive\"\n",
    "\n",
    "                elif callable(self.model):\n",
    "                    if issubclass(type(self.masker), maskers.Independent):\n",
    "                        if self.masker.shape[1] <= 10:\n",
    "                            algorithm = \"exact\"\n",
    "                        else:\n",
    "                            algorithm = \"permutation\"\n",
    "                    elif issubclass(type(self.masker), maskers.Partition):\n",
    "                        if self.masker.shape[1] <= 32:\n",
    "                            algorithm = \"exact\"\n",
    "                        else:\n",
    "                            algorithm = \"permutation\"\n",
    "                    elif issubclass(type(self.masker), maskers.Composite):\n",
    "                        if getattr(self.masker, \"partition_tree\", None) is None:\n",
    "                            algorithm = \"permutation\"\n",
    "                        else:\n",
    "                            algorithm = \"partition\" # TODO: should really only do this if there is more than just tabular\n",
    "                    elif issubclass(type(self.masker), maskers.Image) or issubclass(type(self.masker), maskers.Text) or \\\n",
    "                            issubclass(type(self.masker), maskers.OutputComposite) or issubclass(type(self.masker), maskers.FixedComposite):\n",
    "                        algorithm = \"partition\"\n",
    "                    else:\n",
    "                        algorithm = \"permutation\"\n",
    "\n",
    "                # if we get here then we don't know how to handle what was given to us\n",
    "                else:\n",
    "                    raise Exception(\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: \" + str(model))\n",
    "\n",
    "            # build the right subclass\n",
    "            if algorithm == \"exact\":\n",
    "                self.__class__ = explainers.Exact\n",
    "                explainers.Exact.__init__(self, self.model, self.masker, link=self.link, feature_names=self.feature_names, **kwargs)\n",
    "            elif algorithm == \"permutation\":\n",
    "                self.__class__ = explainers.Permutation\n",
    "                explainers.Permutation.__init__(self, self.model, self.masker, link=self.link, feature_names=self.feature_names, **kwargs)\n",
    "            elif algorithm == \"partition\":\n",
    "                self.__class__ = explainers.Partition\n",
    "                explainers.Partition.__init__(self, self.model, self.masker, link=self.link, feature_names=self.feature_names, output_names=self.output_names, **kwargs)\n",
    "            elif algorithm == \"tree\":\n",
    "                self.__class__ = explainers.Tree\n",
    "                explainers.Tree.__init__(self, self.model, self.masker, link=self.link, feature_names=self.feature_names, **kwargs)\n",
    "            elif algorithm == \"additive\":\n",
    "                self.__class__ = explainers.Additive\n",
    "                explainers.Additive.__init__(self, self.model, self.masker, link=self.link, feature_names=self.feature_names, **kwargs)\n",
    "            elif algorithm == \"linear\":\n",
    "                self.__class__ = explainers.Linear\n",
    "                explainers.Linear.__init__(self, self.model, self.masker, link=self.link, feature_names=self.feature_names, **kwargs)\n",
    "            else:\n",
    "                raise Exception(\"Unknown algorithm type passed: %s!\" % algorithm)\n",
    "\n",
    "\n",
    "    def __call__(self, *args, max_evals=\"auto\", main_effects=False, error_bounds=False, batch_size=\"auto\",\n",
    "                 outputs=None, silent=False, **kwargs):\n",
    "      \n",
    "        if issubclass(type(self.masker), maskers.OutputComposite) and len(args)==2:\n",
    "            self.masker.model = models.TextGeneration(target_sentences=args[1])\n",
    "            args = args[:1]\n",
    "        num_rows = None\n",
    "        args = list(args)\n",
    "        if self.feature_names is None:\n",
    "            feature_names = [None for _ in range(len(args))]\n",
    "        elif issubclass(type(self.feature_names[0]), (list, tuple)):\n",
    "            feature_names = copy.deepcopy(self.feature_names)\n",
    "        else:\n",
    "            feature_names = [copy.deepcopy(self.feature_names)]\n",
    "        for i in range(len(args)):\n",
    "\n",
    "           \n",
    "            if num_rows is None:\n",
    "                try:\n",
    "                    num_rows = len(args[i])\n",
    "                except Exception: \n",
    "                    pass\n",
    "\n",
    "            if safe_isinstance(args[i], \"pandas.core.frame.DataFrame\"):\n",
    "                feature_names[i] = list(args[i].columns)\n",
    "                args[i] = args[i].to_numpy()\n",
    "\n",
    "            if safe_isinstance(args[i], \"nlp.arrow_dataset.Dataset\"):\n",
    "                args[i] = args[i][\"text\"]\n",
    "            elif issubclass(type(args[i]), dict) and \"text\" in args[i]:\n",
    "                args[i] = args[i][\"text\"]\n",
    "\n",
    "        if batch_size == \"auto\":\n",
    "            if hasattr(self.masker, \"default_batch_size\"):\n",
    "                batch_size = self.masker.default_batch_size\n",
    "            else:\n",
    "                batch_size = 10\n",
    "\n",
    "        values = []\n",
    "        output_indices = []\n",
    "        expected_values = []\n",
    "        mask_shapes = []\n",
    "        main_effects = []\n",
    "        hierarchical_values = []\n",
    "        clustering = []\n",
    "        output_names = []\n",
    "        if callable(getattr(self.masker, \"feature_names\", None)):\n",
    "            feature_names = [[] for _ in range(len(args))]\n",
    "        for row_args in show_progress(zip(*args), num_rows, self.__class__.__name__+\" explainer\", silent):\n",
    "            row_result = self.explain_row(\n",
    "                *row_args, max_evals=max_evals, main_effects=main_effects, error_bounds=error_bounds,\n",
    "                batch_size=batch_size, outputs=outputs, silent=silent, **kwargs\n",
    "            )\n",
    "            values.append(row_result.get(\"values\", None))\n",
    "            output_indices.append(row_result.get(\"output_indices\", None))\n",
    "            expected_values.append(row_result.get(\"expected_values\", None))\n",
    "            mask_shapes.append(row_result[\"mask_shapes\"])\n",
    "            main_effects.append(row_result.get(\"main_effects\", None))\n",
    "            clustering.append(row_result.get(\"clustering\", None))\n",
    "            hierarchical_values.append(row_result.get(\"hierarchical_values\", None))\n",
    "            output_names.append(row_result.get(\"output_names\", None))\n",
    "\n",
    "            if callable(getattr(self.masker, \"feature_names\", None)):\n",
    "                row_feature_names = self.masker.feature_names(*row_args)\n",
    "                for i in range(len(row_args)):\n",
    "                    feature_names[i].append(row_feature_names[i])\n",
    "\n",
    "        arg_values = [[] for a in args]\n",
    "        for i, v in enumerate(values):\n",
    "            pos = 0\n",
    "            for j in range(len(args)):\n",
    "                mask_length = np.prod(mask_shapes[i][j])\n",
    "                arg_values[j].append(values[i][pos:pos+mask_length])\n",
    "                pos += mask_length\n",
    "\n",
    "        expected_values = pack_values(expected_values)\n",
    "        main_effects = pack_values(main_effects)\n",
    "        output_indices = pack_values(output_indices)\n",
    "        main_effects = pack_values(main_effects)\n",
    "        hierarchical_values = pack_values(hierarchical_values)\n",
    "        clustering = pack_values(clustering)\n",
    "\n",
    "        ragged_outputs = False\n",
    "        if output_indices is not None:\n",
    "            ragged_outputs = not all(len(x) == len(output_indices[0]) for x in output_indices)\n",
    "        if self.output_names is None:\n",
    "            if None not in output_names:\n",
    "                if not ragged_outputs:\n",
    "                    sliced_labels = np.array(output_names)\n",
    "                else:\n",
    "                    sliced_labels = [np.array(output_names[i])[index_list] for i,index_list in enumerate(output_indices)]\n",
    "            else:\n",
    "                sliced_labels = None\n",
    "        else:\n",
    "            labels = np.array(self.output_names)\n",
    "            sliced_labels = [labels[index_list] for index_list in output_indices]\n",
    "            if not ragged_outputs:\n",
    "                sliced_labels = np.array(sliced_labels)\n",
    "\n",
    "        if isinstance(sliced_labels, np.ndarray) and len(sliced_labels.shape) == 2:\n",
    "            if np.all(sliced_labels[0,:] == sliced_labels):\n",
    "                sliced_labels = sliced_labels[0]\n",
    "\n",
    "        out = []\n",
    "        for j in range(len(args)):\n",
    "\n",
    "            tmp = []\n",
    "            for i, v in enumerate(arg_values[j]):\n",
    "                if np.prod(mask_shapes[i][j]) != np.prod(v.shape):\n",
    "                    tmp.append(v.reshape(*mask_shapes[i][j], -1))\n",
    "                else:\n",
    "                    tmp.append(v.reshape(*mask_shapes[i][j]))\n",
    "            arg_values[j] = pack_values(tmp)\n",
    "\n",
    "            if hasattr(self.masker, \"data_transform\"):\n",
    "                data = pack_values([self.masker.data_transform(v) for v in args[j]])\n",
    "            else:\n",
    "                data = args[j]\n",
    "\n",
    "            out.append(Explanation(\n",
    "                arg_values[j], expected_values, data,\n",
    "                feature_names=feature_names[j], main_effects=main_effects,\n",
    "                clustering=clustering,\n",
    "                hierarchical_values=hierarchical_values,\n",
    "                output_names=sliced_labels \n",
    "            ))\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def explain_row(self, *row_args, max_evals, main_effects, error_bounds, outputs, silent, **kwargs):\n",
    "      \n",
    "        \n",
    "        return {}\n",
    "\n",
    "    @staticmethod\n",
    "    def supports_model_with_masker(model, masker):\n",
    "        \n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_main_effects(fm, expected_value, inds):\n",
    "       \n",
    "\n",
    "        # mask each input on in isolation\n",
    "        masks = np.zeros(2*len(inds)-1, dtype=np.int)\n",
    "        last_ind = -1\n",
    "        for i in range(len(inds)):\n",
    "            if i > 0:\n",
    "                masks[2*i - 1] = -last_ind - 1 # turn off the last input\n",
    "            masks[2*i] = inds[i] # turn on this input\n",
    "            last_ind = inds[i]\n",
    "\n",
    "        # compute the main effects for the given indexes\n",
    "        main_effects = fm(masks) - expected_value\n",
    "\n",
    "        # expand the vector to the full input size\n",
    "        expanded_main_effects = np.zeros(len(fm))\n",
    "        for i, ind in enumerate(inds):\n",
    "            expanded_main_effects[ind] = main_effects[i]\n",
    "\n",
    "        return expanded_main_effects\n",
    "\n",
    "    def save(self, out_file, model_saver=\".save\", masker_saver=\".save\"):\n",
    "        super().save(out_file)\n",
    "        with Serializer(out_file, \"shap.Explainer\", version=0) as s:\n",
    "            s.save(\"model\", self.model, model_saver)\n",
    "            s.save(\"masker\", self.masker, masker_saver)\n",
    "            s.save(\"link\", self.link)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, in_file, model_loader=Model.load, masker_loader=Masker.load, instantiate=True):\n",
    "      \n",
    "        if instantiate:\n",
    "            return cls._instantiated_load(in_file, model_loader=model_loader, masker_loader=masker_loader)\n",
    "\n",
    "        kwargs = super().load(in_file, instantiate=False)\n",
    "        with Deserializer(in_file, \"shap.Explainer\", min_version=0, max_version=0) as s:\n",
    "            kwargs[\"model\"] = s.load(\"model\", model_loader)\n",
    "            kwargs[\"masker\"] = s.load(\"masker\", masker_loader)\n",
    "            kwargs[\"link\"] = s.load(\"link\")\n",
    "        return kwargs\n",
    "\n",
    "def pack_values(values):\n",
    "\n",
    "    # collapse the values if we didn't compute them\n",
    "    if values is None or values[0] is None:\n",
    "        return None\n",
    "    elif np.issubdtype(type(values[0]), np.number) or len(np.unique([len(v) for v in values])) == 1:\n",
    "        return np.array(values)\n",
    "    else:\n",
    "        return np.array(values, dtype=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsgCyzU2FqHF"
   },
   "outputs": [],
   "source": [
    "class Deep(Explainer):\n",
    "    def __init__(self, model, data, session=None, learning_phase_flags=None):\n",
    "        \n",
    "        # first, we need to find the framework\n",
    "        if type(model) is tuple:\n",
    "            a, b = model\n",
    "            try:\n",
    "                a.named_parameters()\n",
    "                framework = 'pytorch'\n",
    "            except:\n",
    "                framework = 'tensorflow'\n",
    "        else:\n",
    "            try:\n",
    "                model.named_parameters()\n",
    "                framework = 'pytorch'\n",
    "            except:\n",
    "                framework = 'tensorflow'\n",
    "\n",
    "        if framework == 'tensorflow':\n",
    "            self.explainer = TFDeep(model, data, session, learning_phase_flags)\n",
    "        elif framework == 'pytorch':\n",
    "            self.explainer = PyTorchDeep(model, data)\n",
    "\n",
    "        self.expected_value = self.explainer.expected_value\n",
    "\n",
    "    def shap_values(self, X, ranked_outputs=None, output_rank_order='max', check_additivity=True):\n",
    "        \n",
    "        return self.explainer.shap_values(X, ranked_outputs, output_rank_order, check_additivity=check_additivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rV8HxkEeFXHA"
   },
   "source": [
    "## Shap Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjuIzHfqKeUl",
    "outputId": "700dabf5-1785-4f24-ef7f-1ec61a2156a1"
   },
   "outputs": [],
   "source": [
    "# since shuffle=True, this is a random sample of test data\n",
    "batch = next(iter(test_loader))\n",
    "images, _ = batch\n",
    "\n",
    "background = images[:100]\n",
    "test_images = images[100:103]\n",
    "\n",
    "e = shap.DeepExplainer(model, background)\n",
    "shap_values = e.shap_values(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tc3SDif_FOts"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5u90n6AKjXN"
   },
   "outputs": [],
   "source": [
    "shap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]\n",
    "test_numpy = np.swapaxes(np.swapaxes(test_images.numpy(), 1, -1), 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "a2I9sjihKnZl",
    "outputId": "bd42dcee-0eb3-49f4-e1b8-0351f3bfbbe6"
   },
   "outputs": [],
   "source": [
    "shap.image_plot(shap_numpy, -test_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7jZGK4NMwQn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST_Kernelshap.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0fc287afa5384d7aa464619883cc08f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "10b441d4c27f467993b67fdca75dd399": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10c89159f8414dd2a036473713abb5b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_333b2f46a376423a83013b5ecd6cb304",
       "IPY_MODEL_46e7b7a9f7b142c3a88fa5a3a4e35058"
      ],
      "layout": "IPY_MODEL_1eb65b30582a4f6d91b7d21e73783654"
     }
    },
    "1eb65b30582a4f6d91b7d21e73783654": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24a56742a9044889b9cdb7a42fe264dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10b441d4c27f467993b67fdca75dd399",
      "placeholder": "​",
      "style": "IPY_MODEL_a55209163302433b92a96cf1f2210f6d",
      "value": " 29696/? [00:37&lt;00:00, 785.75it/s]"
     }
    },
    "2f65be33b22047ba8a1fbe0229834852": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93eb8b24373a4ef59b39e288c85a085f",
      "max": 1648877,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_added4c7e1b14cd78fa3b6edb050fc03",
      "value": 1648877
     }
    },
    "32b515b77c064a6681baabf531b05e34": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "333b2f46a376423a83013b5ecd6cb304": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed825f77af394645a35a35b99fedf81e",
      "max": 4542,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5153c3d3005b4990802e72c1b1dc7d5e",
      "value": 4542
     }
    },
    "36e700da083b46309ce215f187afbf7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd59ca81351e4b3e82dd06e06d0349ba",
      "max": 28881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e1228a1cb76f44c18fee5cdfad29054d",
      "value": 28881
     }
    },
    "46e7b7a9f7b142c3a88fa5a3a4e35058": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_837c1cd7823342baa7d2b8a0085109b4",
      "placeholder": "​",
      "style": "IPY_MODEL_a6d0dfa85f1d4df9abcab9e2f9a9297e",
      "value": " 5120/? [00:00&lt;00:00, 50480.80it/s]"
     }
    },
    "4a6bb86811a747839816b949447b4565": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5153c3d3005b4990802e72c1b1dc7d5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5304c1dbb8fd4b01b48cb123605007a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0e12e9ad0554075a10c72a7dd7afb17",
      "max": 9912422,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0fc287afa5384d7aa464619883cc08f1",
      "value": 9912422
     }
    },
    "65c09095efba45e59e624108a61ae109": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6af8cc8d8b404730a0f9f43f22cdef71": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b9fa0e50d8d4706bf727f39ba231fa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "837c1cd7823342baa7d2b8a0085109b4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d1f7b58788542fb9620238c68601c74": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93eb8b24373a4ef59b39e288c85a085f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cc6522b757646cfb64c73f033e88c12": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc14d94f56554bb6bdf9aa3d465abc84",
      "placeholder": "​",
      "style": "IPY_MODEL_7b9fa0e50d8d4706bf727f39ba231fa0",
      "value": " 9913344/? [09:14&lt;00:00, 17894.11it/s]"
     }
    },
    "a4bfa0f9274a4cf6a26ef72f8573e4a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6af8cc8d8b404730a0f9f43f22cdef71",
      "placeholder": "​",
      "style": "IPY_MODEL_32b515b77c064a6681baabf531b05e34",
      "value": " 1649664/? [00:37&lt;00:00, 43841.10it/s]"
     }
    },
    "a55209163302433b92a96cf1f2210f6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6d0dfa85f1d4df9abcab9e2f9a9297e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "added4c7e1b14cd78fa3b6edb050fc03": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "aee4a61cdbb141ee895838a410fc48ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2f65be33b22047ba8a1fbe0229834852",
       "IPY_MODEL_a4bfa0f9274a4cf6a26ef72f8573e4a0"
      ],
      "layout": "IPY_MODEL_8d1f7b58788542fb9620238c68601c74"
     }
    },
    "ba7111f863aa4ee58d778817bc2529f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_36e700da083b46309ce215f187afbf7f",
       "IPY_MODEL_24a56742a9044889b9cdb7a42fe264dc"
      ],
      "layout": "IPY_MODEL_65c09095efba45e59e624108a61ae109"
     }
    },
    "d2dcfbcba04b46ddb9984361dd86fd2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5304c1dbb8fd4b01b48cb123605007a4",
       "IPY_MODEL_9cc6522b757646cfb64c73f033e88c12"
      ],
      "layout": "IPY_MODEL_4a6bb86811a747839816b949447b4565"
     }
    },
    "e0e12e9ad0554075a10c72a7dd7afb17": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1228a1cb76f44c18fee5cdfad29054d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ed825f77af394645a35a35b99fedf81e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc14d94f56554bb6bdf9aa3d465abc84": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd59ca81351e4b3e82dd06e06d0349ba": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}