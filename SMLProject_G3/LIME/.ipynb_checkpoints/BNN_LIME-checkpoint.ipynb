{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BNN_LIME.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "D1lCt8D1F3df"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLqjMDatANXY",
        "outputId": "454d1c21-3cd8-4700-e1f3-2f96732b4a16"
      },
      "source": [
        "!git clone https://github.com/Harry24k/bayesian-neural-network-pytorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bayesian-neural-network-pytorch'...\n",
            "remote: Enumerating objects: 274, done.\u001b[K\n",
            "remote: Counting objects: 100% (274/274), done.\u001b[K\n",
            "remote: Compressing objects: 100% (179/179), done.\u001b[K\n",
            "remote: Total 274 (delta 123), reused 231 (delta 82), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (274/274), 22.76 MiB | 29.20 MiB/s, done.\n",
            "Resolving deltas: 100% (123/123), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHt6k3IzAdOV"
      },
      "source": [
        "import os\n",
        "os.chdir('bayesian-neural-network-pytorch')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH45M24p_eS-"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchbnn as bnn"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwpuu1Ej_l2u"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZzRBrNtBKNU"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9pjLX2fddLm"
      },
      "source": [
        "batch_size = 256\n",
        "DEVICE = 'cpu'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfa1yHiJ_oz-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29fb8612-0aa5-42c8-db0f-f50a319924d7"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('mnist-data/', train=True, download=True,\n",
        "                       transform=transforms.Compose([transforms.ToTensor(),])),\n",
        "        batch_size=batch_size, shuffle=True)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist-data/MNIST/raw/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to mnist-data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38ca470a67fc4cb7a097611c79b8539a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting mnist-data/MNIST/raw/train-images-idx3-ubyte.gz to mnist-data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to mnist-data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09a1f811949545658db3ad6202aab855",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting mnist-data/MNIST/raw/train-labels-idx1-ubyte.gz to mnist-data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to mnist-data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcbea37f721f43a485f7e68f95f83250",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting mnist-data/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist-data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist-data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d476cb99fc7645488dfdae1d0fa5db12",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting mnist-data/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist-data/MNIST/raw\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1hkJmlUdfkD"
      },
      "source": [
        "test_set = datasets.MNIST('mnist-data/', train=False, transform=transforms.Compose([transforms.ToTensor(),]))\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2afIGAI_sDe"
      },
      "source": [
        "model = nn.Sequential(\n",
        "    bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=28*28, out_features=1024),\n",
        "    nn.ReLU(),\n",
        "    bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=1024, out_features=10),\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OJQSU9M_teX"
      },
      "source": [
        "model = model.to(DEVICE)\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
        "kl_weight = 0.01\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45YKAQzl_vU4"
      },
      "source": [
        "kl_weight = 0.1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aW4PaJq_w6H",
        "outputId": "b3851108-c0b0-4ba2-825f-f5ea297fcf5b"
      },
      "source": [
        "for step in range(6):\n",
        "    running_cost =0\n",
        "    N = 0\n",
        "    for i, batch in enumerate(train_loader):\n",
        "      x, y = batch\n",
        "      bs = x.shape[0]\n",
        "      x = x.reshape(bs,-1)\n",
        "      x = x.to(DEVICE)\n",
        "      y = y.to(DEVICE)\n",
        "      # print(x.shape,y.shape)\n",
        "      pre = model(x)\n",
        "      ce = ce_loss(pre, y)\n",
        "      kl = kl_loss(model)\n",
        "      cost = ce + kl_weight*kl\n",
        "      running_cost = cost.item()\n",
        "      N+=1\n",
        "      optimizer.zero_grad()\n",
        "      cost.backward()\n",
        "      optimizer.step()\n",
        "    print('Epoch:', step+1, 'Cost:', running_cost/N)\n",
        "    "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 Cost: 0.0009711909801401991\n",
            "Epoch: 2 Cost: 0.0006592283857629654\n",
            "Epoch: 3 Cost: 0.0009064897577813331\n",
            "Epoch: 4 Cost: 0.0009225672229807427\n",
            "Epoch: 5 Cost: 0.00047905739951641\n",
            "Epoch: 6 Cost: 0.00023855788910642583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkJi5rWsEAyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "195b0d13-f814-4b95-d5e5-43b6f53e7438"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "for j, batch in enumerate(test_loader):\n",
        "    x, y = batch\n",
        "    bs = x.shape[0]\n",
        "    x = x.reshape(bs,-1)\n",
        "    x = x.to(DEVICE)\n",
        "    y = y.to(DEVICE)\n",
        "    # print(x.shape,y.shape)\n",
        "    pre = model(x)\n",
        "    _, predicted = torch.max(pre.data, 1)\n",
        "    t = y.size(0)\n",
        "    c = (predicted == y).sum()\n",
        "    total += t\n",
        "    correct += c\n",
        "print(\"accuracy: %d %%\" % (100 * correct / total))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 96 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1lCt8D1F3df"
      },
      "source": [
        "# LIME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSfpxUNXAsAs"
      },
      "source": [
        "import sys\n",
        "import inspect\n",
        "def has_arg(fn, arg_name):\n",
        "    \"\"\"Checks if a callable accepts a given keyword argument.\n",
        "    Args:\n",
        "        fn: callable to inspect\n",
        "        arg_name: string, keyword argument name to check\n",
        "    Returns:\n",
        "        bool, whether `fn` accepts a `arg_name` keyword argument.\n",
        "    \"\"\"\n",
        "    if sys.version_info < (3,):\n",
        "        if isinstance(fn, types.FunctionType) or isinstance(fn, types.MethodType):\n",
        "            arg_spec = inspect.getargspec(fn)\n",
        "        else:\n",
        "            try:\n",
        "                arg_spec = inspect.getargspec(fn.__call__)\n",
        "            except AttributeError:\n",
        "                return False\n",
        "        return (arg_name in arg_spec.args)\n",
        "    elif sys.version_info < (3, 6):\n",
        "        arg_spec = inspect.getfullargspec(fn)\n",
        "        return (arg_name in arg_spec.args or\n",
        "                arg_name in arg_spec.kwonlyargs)\n",
        "    else:\n",
        "        try:\n",
        "            signature = inspect.signature(fn)\n",
        "        except ValueError:\n",
        "            # handling Cython\n",
        "            signature = inspect.signature(fn.__call__)\n",
        "        parameter = signature.parameters.get(arg_name)\n",
        "        if parameter is None:\n",
        "            return False\n",
        "        return (parameter.kind in (inspect.Parameter.POSITIONAL_OR_KEYWORD,\n",
        "                                   inspect.Parameter.KEYWORD_ONLY))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zazm11pMAsqg"
      },
      "source": [
        "import types\n",
        "\n",
        "from skimage.segmentation import felzenszwalb, slic, quickshift\n",
        "\n",
        "\n",
        "class BaseWrapper(object):\n",
        "    \"\"\"Base class for LIME Scikit-Image wrapper\n",
        "\n",
        "\n",
        "    Args:\n",
        "        target_fn: callable function or class instance\n",
        "        target_params: dict, parameters to pass to the target_fn\n",
        "\n",
        "\n",
        "    'target_params' takes parameters required to instanciate the\n",
        "        desired Scikit-Image class/model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, target_fn=None, **target_params):\n",
        "        self.target_fn = target_fn\n",
        "        self.target_params = target_params\n",
        "\n",
        "    def _check_params(self, parameters):\n",
        "        \"\"\"Checks for mistakes in 'parameters'\n",
        "\n",
        "        Args :\n",
        "            parameters: dict, parameters to be checked\n",
        "\n",
        "        Raises :\n",
        "            ValueError: if any parameter is not a valid argument for the target function\n",
        "                or the target function is not defined\n",
        "            TypeError: if argument parameters is not iterable\n",
        "         \"\"\"\n",
        "        a_valid_fn = []\n",
        "        if self.target_fn is None:\n",
        "            if callable(self):\n",
        "                a_valid_fn.append(self.__call__)\n",
        "            else:\n",
        "                raise TypeError('invalid argument: tested object is not callable,\\\n",
        "                 please provide a valid target_fn')\n",
        "        elif isinstance(self.target_fn, types.FunctionType) \\\n",
        "                or isinstance(self.target_fn, types.MethodType):\n",
        "            a_valid_fn.append(self.target_fn)\n",
        "        else:\n",
        "            a_valid_fn.append(self.target_fn.__call__)\n",
        "\n",
        "        if not isinstance(parameters, str):\n",
        "            for p in parameters:\n",
        "                for fn in a_valid_fn:\n",
        "                    if has_arg(fn, p):\n",
        "                        pass\n",
        "                    else:\n",
        "                        raise ValueError('{} is not a valid parameter'.format(p))\n",
        "        else:\n",
        "            raise TypeError('invalid argument: list or dictionnary expected')\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        \"\"\"Sets the parameters of this estimator.\n",
        "        Args:\n",
        "            **params: Dictionary of parameter names mapped to their values.\n",
        "\n",
        "        Raises :\n",
        "            ValueError: if any parameter is not a valid argument\n",
        "                for the target function\n",
        "        \"\"\"\n",
        "        self._check_params(params)\n",
        "        self.target_params = params\n",
        "\n",
        "    def filter_params(self, fn, override=None):\n",
        "        \"\"\"Filters `target_params` and return those in `fn`'s arguments.\n",
        "        Args:\n",
        "            fn : arbitrary function\n",
        "            override: dict, values to override target_params\n",
        "        Returns:\n",
        "            result : dict, dictionary containing variables\n",
        "            in both target_params and fn's arguments.\n",
        "        \"\"\"\n",
        "        override = override or {}\n",
        "        result = {}\n",
        "        for name, value in self.target_params.items():\n",
        "            if has_arg(fn, name):\n",
        "                result.update({name: value})\n",
        "        result.update(override)\n",
        "        return result\n",
        "\n",
        "\n",
        "class SegmentationAlgorithm(BaseWrapper):\n",
        "    \"\"\" Define the image segmentation function based on Scikit-Image\n",
        "            implementation and a set of provided parameters\n",
        "\n",
        "        Args:\n",
        "            algo_type: string, segmentation algorithm among the following:\n",
        "                'quickshift', 'slic', 'felzenszwalb'\n",
        "            target_params: dict, algorithm parameters (valid model paramters\n",
        "                as define in Scikit-Image documentation)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, algo_type, **target_params):\n",
        "        self.algo_type = algo_type\n",
        "        if (self.algo_type == 'quickshift'):\n",
        "            BaseWrapper.__init__(self, quickshift, **target_params)\n",
        "            kwargs = self.filter_params(quickshift)\n",
        "            self.set_params(**kwargs)\n",
        "        elif (self.algo_type == 'felzenszwalb'):\n",
        "            BaseWrapper.__init__(self, felzenszwalb, **target_params)\n",
        "            kwargs = self.filter_params(felzenszwalb)\n",
        "            self.set_params(**kwargs)\n",
        "        elif (self.algo_type == 'slic'):\n",
        "            BaseWrapper.__init__(self, slic, **target_params)\n",
        "            kwargs = self.filter_params(slic)\n",
        "            self.set_params(**kwargs)\n",
        "\n",
        "    def __call__(self, *args):\n",
        "        return self.target_fn(args[0], **self.target_params)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUpxq_rFA2yk"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn.linear_model import Ridge, lars_path\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "\n",
        "class LimeBase(object):\n",
        "    \"\"\"Class for learning a locally linear sparse model from perturbed data\"\"\"\n",
        "    def __init__(self,\n",
        "                 kernel_fn,\n",
        "                 verbose=False,\n",
        "                 random_state=None):\n",
        "\n",
        "        self.kernel_fn = kernel_fn\n",
        "        self.verbose = verbose\n",
        "        self.random_state = check_random_state(random_state)\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_lars_path(weighted_data, weighted_labels):\n",
        "\n",
        "        x_vector = weighted_data\n",
        "        alphas, _, coefs = lars_path(x_vector,\n",
        "                                     weighted_labels,\n",
        "                                     method='lasso',\n",
        "                                     verbose=False)\n",
        "        return alphas, coefs\n",
        "\n",
        "    def forward_selection(self, data, labels, weights, num_features):\n",
        "        \"\"\"Iteratively adds features to the model\"\"\"\n",
        "        clf = Ridge(alpha=0, fit_intercept=True, random_state=self.random_state)\n",
        "        clf.verbose = False\n",
        "        used_features = []\n",
        "        for _ in range(min(num_features, data.shape[1])):\n",
        "            max_ = -100000000\n",
        "            best = 0\n",
        "            for feature in range(data.shape[1]):\n",
        "                if feature in used_features:\n",
        "                    continue\n",
        "                clf.fit(data[:, used_features + [feature]], labels,\n",
        "                        sample_weight=weights)\n",
        "                score = clf.score(data[:, used_features + [feature]],\n",
        "                                  labels,\n",
        "                                  sample_weight=weights)\n",
        "                if score > max_:\n",
        "                    best = feature\n",
        "                    max_ = score\n",
        "            used_features.append(best)\n",
        "        return np.array(used_features)\n",
        "\n",
        "    def feature_selection(self, data, labels, weights, num_features, method):\n",
        "        if method == 'none':\n",
        "            return np.array(range(data.shape[1]))\n",
        "        elif method == 'forward_selection':\n",
        "            return self.forward_selection(data, labels, weights, num_features)\n",
        "        elif method == 'highest_weights':\n",
        "            clf = Ridge(alpha=0.01, fit_intercept=True,\n",
        "                        random_state=self.random_state)\n",
        "            clf.verbose = False\n",
        "            clf.fit(data, labels, sample_weight=weights)\n",
        "\n",
        "            coef = clf.coef_\n",
        "            if sp.sparse.issparse(data):\n",
        "                coef = sp.sparse.csr_matrix(clf.coef_)\n",
        "                weighted_data = coef.multiply(data[0])\n",
        "                # Note: most efficient to slice the data before reversing\n",
        "                sdata = len(weighted_data.data)\n",
        "                argsort_data = np.abs(weighted_data.data).argsort()\n",
        "                # Edge case where data is more sparse than requested number of feature importances\n",
        "                # In that case, we just pad with zero-valued features\n",
        "                if sdata < num_features:\n",
        "                    nnz_indexes = argsort_data[::-1]\n",
        "                    indices = weighted_data.indices[nnz_indexes]\n",
        "                    num_to_pad = num_features - sdata\n",
        "                    indices = np.concatenate((indices, np.zeros(num_to_pad, dtype=indices.dtype)))\n",
        "                    indices_set = set(indices)\n",
        "                    pad_counter = 0\n",
        "                    for i in range(data.shape[1]):\n",
        "                        if i not in indices_set:\n",
        "                            indices[pad_counter + sdata] = i\n",
        "                            pad_counter += 1\n",
        "                            if pad_counter >= num_to_pad:\n",
        "                                break\n",
        "                else:\n",
        "                    nnz_indexes = argsort_data[sdata - num_features:sdata][::-1]\n",
        "                    indices = weighted_data.indices[nnz_indexes]\n",
        "                return indices\n",
        "            else:\n",
        "                weighted_data = coef * data[0]\n",
        "                feature_weights = sorted(\n",
        "                    zip(range(data.shape[1]), weighted_data),\n",
        "                    key=lambda x: np.abs(x[1]),\n",
        "                    reverse=True)\n",
        "                return np.array([x[0] for x in feature_weights[:num_features]])\n",
        "        elif method == 'lasso_path':\n",
        "            weighted_data = ((data - np.average(data, axis=0, weights=weights))\n",
        "                             * np.sqrt(weights[:, np.newaxis]))\n",
        "            weighted_labels = ((labels - np.average(labels, weights=weights))\n",
        "                               * np.sqrt(weights))\n",
        "            nonzero = range(weighted_data.shape[1])\n",
        "            _, coefs = self.generate_lars_path(weighted_data,\n",
        "                                               weighted_labels)\n",
        "            for i in range(len(coefs.T) - 1, 0, -1):\n",
        "                nonzero = coefs.T[i].nonzero()[0]\n",
        "                if len(nonzero) <= num_features:\n",
        "                    break\n",
        "            used_features = nonzero\n",
        "            return used_features\n",
        "        elif method == 'auto':\n",
        "            if num_features <= 6:\n",
        "                n_method = 'forward_selection'\n",
        "            else:\n",
        "                n_method = 'highest_weights'\n",
        "            return self.feature_selection(data, labels, weights,\n",
        "                                          num_features, n_method)\n",
        "\n",
        "    def explain_instance_with_data(self,\n",
        "                                   neighborhood_data,\n",
        "                                   neighborhood_labels,\n",
        "                                   distances,\n",
        "                                   label,\n",
        "                                   num_features,\n",
        "                                   feature_selection='auto',\n",
        "                                   model_regressor=None):\n",
        "      \n",
        "        weights = self.kernel_fn(distances)\n",
        "        labels_column = neighborhood_labels[:, label]\n",
        "        used_features = self.feature_selection(neighborhood_data,\n",
        "                                               labels_column,\n",
        "                                               weights,\n",
        "                                               num_features,\n",
        "                                               feature_selection)\n",
        "        if model_regressor is None:\n",
        "            model_regressor = Ridge(alpha=1, fit_intercept=True,\n",
        "                                    random_state=self.random_state)\n",
        "        easy_model = model_regressor\n",
        "        easy_model.fit(neighborhood_data[:, used_features],\n",
        "                       labels_column, sample_weight=weights)\n",
        "        prediction_score = easy_model.score(\n",
        "            neighborhood_data[:, used_features],\n",
        "            labels_column, sample_weight=weights)\n",
        "\n",
        "        local_pred = easy_model.predict(neighborhood_data[0, used_features].reshape(1, -1))\n",
        "\n",
        "        if self.verbose:\n",
        "            print('Intercept', easy_model.intercept_)\n",
        "            print('Prediction_local', local_pred,)\n",
        "            print('Right:', neighborhood_labels[0, label])\n",
        "        return (easy_model.intercept_,\n",
        "                sorted(zip(used_features, easy_model.coef_),\n",
        "                       key=lambda x: np.abs(x[1]), reverse=True),\n",
        "                prediction_score, local_pred)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lx1WRk5A4s2"
      },
      "source": [
        "import copy\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.utils import check_random_state\n",
        "from skimage.color import gray2rgb\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ImageExplanation(object):\n",
        "    def __init__(self, image, segments):\n",
        "        self.image = image\n",
        "        self.segments = segments\n",
        "        self.intercept = {}\n",
        "        self.local_exp = {}\n",
        "        self.local_pred = {}\n",
        "        self.score = {}\n",
        "\n",
        "    def get_image_and_mask(self, label, positive_only=True, negative_only=False, hide_rest=False,\n",
        "                           num_features=5, min_weight=0.):\n",
        "        if label not in self.local_exp:\n",
        "            raise KeyError('Label not in explanation')\n",
        "        if positive_only & negative_only:\n",
        "            raise ValueError(\"Positive_only and negative_only cannot be true at the same time.\")\n",
        "        segments = self.segments\n",
        "        image = self.image\n",
        "        exp = self.local_exp[label]\n",
        "        mask = np.zeros(segments.shape, segments.dtype)\n",
        "        if hide_rest:\n",
        "            temp = np.zeros(self.image.shape)\n",
        "        else:\n",
        "            temp = self.image.copy()\n",
        "        if positive_only:\n",
        "            fs = [x[0] for x in exp\n",
        "                  if x[1] > 0 and x[1] > min_weight][:num_features]\n",
        "        if negative_only:\n",
        "            fs = [x[0] for x in exp\n",
        "                  if x[1] < 0 and abs(x[1]) > min_weight][:num_features]\n",
        "        if positive_only or negative_only:\n",
        "            for f in fs:\n",
        "                temp[segments == f] = image[segments == f].copy()\n",
        "                mask[segments == f] = 1\n",
        "            return temp, mask\n",
        "        else:\n",
        "            for f, w in exp[:num_features]:\n",
        "                if np.abs(w) < min_weight:\n",
        "                    continue\n",
        "                c = 0 if w < 0 else 1\n",
        "                mask[segments == f] = -1 if w < 0 else 1\n",
        "                temp[segments == f] = image[segments == f].copy()\n",
        "                temp[segments == f, c] = np.max(image)\n",
        "            return temp, mask\n",
        "\n",
        "\n",
        "class LimeImageExplainer(object):\n",
        "    def __init__(self, kernel_width=.25, kernel=None, verbose=False,\n",
        "                 feature_selection='auto', random_state=None):\n",
        "        kernel_width = float(kernel_width)\n",
        "\n",
        "        if kernel is None:\n",
        "            def kernel(d, kernel_width):\n",
        "                return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
        "\n",
        "        kernel_fn = partial(kernel, kernel_width=kernel_width)\n",
        "\n",
        "        self.random_state = check_random_state(random_state)\n",
        "        self.feature_selection = feature_selection\n",
        "        self.base = LimeBase(kernel_fn, verbose, random_state=self.random_state)\n",
        "\n",
        "    def explain_instance(self, image, classifier_fn, labels=(1,),\n",
        "                         hide_color=None,\n",
        "                         top_labels=5, num_features=100000, num_samples=1000,\n",
        "                         batch_size=10,\n",
        "                         segmentation_fn=None,\n",
        "                         distance_metric='cosine',\n",
        "                         model_regressor=None,\n",
        "                         random_seed=None,\n",
        "                         progress_bar=True):\n",
        "        \n",
        "        if len(image.shape) == 2:\n",
        "            image = gray2rgb(image)\n",
        "        if random_seed is None:\n",
        "            random_seed = self.random_state.randint(0, high=1000)\n",
        "\n",
        "        if segmentation_fn is None:\n",
        "            segmentation_fn = SegmentationAlgorithm('quickshift', kernel_size=4,\n",
        "                                                    max_dist=200, ratio=0.2,\n",
        "                                                    random_seed=random_seed)\n",
        "        segments = segmentation_fn(image)\n",
        "\n",
        "        fudged_image = image.copy()\n",
        "        if hide_color is None:\n",
        "            for x in np.unique(segments):\n",
        "                fudged_image[segments == x] = (\n",
        "                    np.mean(image[segments == x][:, 0]),\n",
        "                    np.mean(image[segments == x][:, 1]),\n",
        "                    np.mean(image[segments == x][:, 2]))\n",
        "        else:\n",
        "            fudged_image[:] = hide_color\n",
        "\n",
        "        top = labels\n",
        "\n",
        "        data, labels = self.data_labels(image, fudged_image, segments,\n",
        "                                        classifier_fn, num_samples,\n",
        "                                        batch_size=batch_size,\n",
        "                                        progress_bar=progress_bar)\n",
        "\n",
        "        distances = sklearn.metrics.pairwise_distances(\n",
        "            data,\n",
        "            data[0].reshape(1, -1),\n",
        "            metric=distance_metric\n",
        "        ).ravel()\n",
        "\n",
        "        ret_exp = ImageExplanation(image, segments)\n",
        "        if top_labels:\n",
        "            top = np.argsort(labels[0])[-top_labels:]\n",
        "            ret_exp.top_labels = list(top)\n",
        "            ret_exp.top_labels.reverse()\n",
        "        for label in top:\n",
        "            (ret_exp.intercept[label],\n",
        "             ret_exp.local_exp[label],\n",
        "             ret_exp.score[label],\n",
        "             ret_exp.local_pred[label]) = self.base.explain_instance_with_data(\n",
        "                data, labels, distances, label, num_features,\n",
        "                model_regressor=model_regressor,\n",
        "                feature_selection=self.feature_selection)\n",
        "        return ret_exp\n",
        "\n",
        "    def data_labels(self,\n",
        "                    image,\n",
        "                    fudged_image,\n",
        "                    segments,\n",
        "                    classifier_fn,\n",
        "                    num_samples,\n",
        "                    batch_size=10,\n",
        "                    progress_bar=True):\n",
        "    \n",
        "        n_features = np.unique(segments).shape[0]\n",
        "        data = self.random_state.randint(0, 2, num_samples * n_features)\\\n",
        "            .reshape((num_samples, n_features))\n",
        "        labels = []\n",
        "        data[0, :] = 1\n",
        "        imgs = []\n",
        "        rows = tqdm(data) if progress_bar else data\n",
        "        for row in rows:\n",
        "            temp = copy.deepcopy(image)\n",
        "            zeros = np.where(row == 0)[0]\n",
        "            mask = np.zeros(segments.shape).astype(bool)\n",
        "            for z in zeros:\n",
        "                mask[segments == z] = True\n",
        "            temp[mask] = fudged_image[mask]\n",
        "            imgs.append(temp)\n",
        "            if len(imgs) == batch_size:\n",
        "                preds = classifier_fn(np.array(imgs))\n",
        "                labels.extend(preds)\n",
        "                imgs = []\n",
        "        if len(imgs) > 0:\n",
        "            preds = classifier_fn(np.array(imgs))\n",
        "            labels.extend(preds)\n",
        "        return data, np.array(labels)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZJJNIx4A7kO"
      },
      "source": [
        "def get_pil_transform(): \n",
        "    transf = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.CenterCrop(224)\n",
        "    ])    \n",
        "\n",
        "    return transf\n",
        "\n",
        "def get_preprocess_transform():\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])     \n",
        "    transf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])    \n",
        "\n",
        "    return transf    \n",
        "\n",
        "pill_transf = get_pil_transform()\n",
        "preprocess_transform = get_preprocess_transform()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzK5hlspBCiE"
      },
      "source": [
        "# Explaining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7B5QsGsBzM_",
        "outputId": "bbf63c2e-cb69-450b-e97c-dd0d5d796996"
      },
      "source": [
        "images = test_set.data\n",
        "labels = test_set.targets\n",
        "img = images[0,:,:]\n",
        "print(images.shape)\n",
        "plt.imshow(img)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10000, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0dc66170d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcVUNyzAhlcT"
      },
      "source": [
        "## batch_predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu8l1yZvA8Hj"
      },
      "source": [
        "def batch_predict(images):\n",
        "    model.eval()\n",
        "    batch = torch.stack(tuple(preprocess_transform(i) for i in images), dim=0)\n",
        "\n",
        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # model.to(DEVICE)\n",
        "    batch = batch.to(DEVICE)\n",
        "    # bs = batch.shape[0]\n",
        "    # print(batch.shape)\n",
        "    batch=  batch[:,0,:,:].reshape(-1,28*28)\n",
        "    # print(batch.shape)\n",
        "    logits = model(batch)\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    return probs.detach().cpu().numpy()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2fJYAZqBOnL"
      },
      "source": [
        "from skimage.segmentation import mark_boundaries\n",
        "import random\n",
        "\n",
        "\n",
        "segmentation_fn= SegmentationAlgorithm('quickshift', kernel_size=2,\n",
        "                                          max_dist=2, ratio=0.2,\n",
        "                                          random_seed= int(random.random()*1000))\n",
        "\n",
        "\n",
        "# segmentation_fn= SegmentationAlgorithm('felzenszwalb', scale=1.0, sigma=0.3)\n",
        "# segmentation_fn= SegmentationAlgorithm('slic', n_segments=100, compactness=0.1)\n",
        "\n",
        "\n",
        "def explain(img, show=False):\n",
        "  if show:\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "  explainer = LimeImageExplainer()\n",
        "  explanation = explainer.explain_instance(img, \n",
        "                                         batch_predict, # classification function\n",
        "                                         top_labels=1, \n",
        "                                        #  hide_color=0, \n",
        "                                        segmentation_fn = segmentation_fn,\n",
        "                                         num_samples=100 )\n",
        "\n",
        "  temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only= False, num_features=30, hide_rest=False)\n",
        "  img_boundry1 = mark_boundaries(temp/255.0, mask, color=(1, 1, 0))\n",
        "  if show:\n",
        "    # print(i, np.mean(mask), explanation.top_labels[0])\n",
        "    # print(explanation.local_exp[explanation.top_labels[0]])\n",
        "    # print(explanation.segments)\n",
        "    plt.imshow(img_boundry1)\n",
        "    plt.show()\n",
        "    plt.imshow(mask)\n",
        "    plt.show()\n",
        "  return explanation.segments, explanation.local_exp[explanation.top_labels[0]], img_boundry1"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnafO7GjBQOc"
      },
      "source": [
        "import random\n",
        "from google.colab import files\n",
        "\n",
        "N = np.shape(images)[0]\n",
        "samples = []\n",
        "indexes = []\n",
        "import time\n",
        "random.seed(time.time())\n",
        "\n",
        "for i in range(10):\n",
        "  a =  [j for j in range(N) if labels[j]==i]\n",
        "  random.shuffle(a) \n",
        "  indexes.extend(a[:3])\n",
        "\n",
        "# indexes = np.random.permutation(np.shape(images)[0])\n",
        "# print(len(indexes))\n",
        "import os\n",
        "if not os.path.exists('result'):\n",
        "  os.mkdir('result')\n",
        "for i in range(len(indexes)):\n",
        "  img = images[indexes[i],:,:]\n",
        "  segs, epx, img_boundry = explain(img, show=False)\n",
        "  \n",
        "  plt.imshow(img)\n",
        "  label = int(i/3)\n",
        "  plt.savefig('result/bnnmnist_'+str(label)+'_lime_original_'+str(i%3)+'.png')\n",
        "  # files.download('result/mnist_'+str(label)+'_lime_original_'+str(i%3)+'.png')\n",
        "  plt.show()\n",
        "  plt.imshow(img_boundry)\n",
        "  plt.savefig('result/bnnmnist_'+str(label)+'_lime_explain_'+str(i%3)+'.png')\n",
        "  # files.download('result/mnist_'+str(label)+'_lime_explain_'+str(i%3)+'.png')\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSM54D1UwEM3",
        "outputId": "44dcf20c-7eb1-4170-e139-978e98a97e84"
      },
      "source": [
        "! zip -r bnnmnist_lime.zip result"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: result/ (stored 0%)\n",
            "  adding: result/bnnmnist_2_lime_original_1.png (deflated 22%)\n",
            "  adding: result/bnnmnist_6_lime_original_2.png (deflated 19%)\n",
            "  adding: result/bnnmnist_2_lime_explain_2.png (deflated 21%)\n",
            "  adding: result/bnnmnist_6_lime_explain_0.png (deflated 21%)\n",
            "  adding: result/bnnmnist_6_lime_explain_1.png (deflated 22%)\n",
            "  adding: result/bnnmnist_2_lime_explain_1.png (deflated 22%)\n",
            "  adding: result/bnnmnist_8_lime_explain_2.png (deflated 21%)\n",
            "  adding: result/bnnmnist_4_lime_original_1.png (deflated 21%)\n",
            "  adding: result/bnnmnist_0_lime_explain_0.png (deflated 22%)\n",
            "  adding: result/bnnmnist_9_lime_explain_1.png (deflated 22%)\n",
            "  adding: result/bnnmnist_3_lime_original_1.png (deflated 21%)\n",
            "  adding: result/bnnmnist_8_lime_original_2.png (deflated 20%)\n",
            "  adding: result/bnnmnist_5_lime_original_2.png (deflated 19%)\n",
            "  adding: result/bnnmnist_7_lime_explain_0.png (deflated 21%)\n",
            "  adding: result/bnnmnist_1_lime_explain_1.png (deflated 22%)\n",
            "  adding: result/bnnmnist_4_lime_original_2.png (deflated 20%)\n",
            "  adding: result/bnnmnist_5_lime_explain_2.png (deflated 21%)\n",
            "  adding: result/bnnmnist_9_lime_original_0.png (deflated 20%)\n",
            "  adding: result/bnnmnist_5_lime_explain_1.png (deflated 21%)\n",
            "  adding: result/bnnmnist_0_lime_original_2.png (deflated 19%)\n",
            "  adding: result/bnnmnist_9_lime_explain_0.png (deflated 21%)\n",
            "  adding: result/bnnmnist_5_lime_original_0.png (deflated 19%)\n",
            "  adding: result/bnnmnist_3_lime_explain_1.png (deflated 22%)\n",
            "  adding: result/bnnmnist_8_lime_explain_1.png (deflated 21%)\n",
            "  adding: result/bnnmnist_7_lime_explain_1.png (deflated 23%)\n",
            "  adding: result/bnnmnist_9_lime_explain_2.png (deflated 24%)\n",
            "  adding: result/bnnmnist_0_lime_original_0.png (deflated 21%)\n",
            "  adding: result/bnnmnist_0_lime_explain_2.png (deflated 21%)\n",
            "  adding: result/bnnmnist_6_lime_explain_2.png (deflated 21%)\n",
            "  adding: result/bnnmnist_3_lime_original_0.png (deflated 20%)\n",
            "  adding: result/bnnmnist_1_lime_original_1.png (deflated 21%)\n",
            "  adding: result/bnnmnist_1_lime_original_2.png (deflated 22%)\n",
            "  adding: result/bnnmnist_1_lime_explain_2.png (deflated 23%)\n",
            "  adding: result/bnnmnist_7_lime_original_1.png (deflated 20%)\n",
            "  adding: result/bnnmnist_4_lime_explain_2.png (deflated 20%)\n",
            "  adding: result/bnnmnist_4_lime_explain_1.png (deflated 23%)\n",
            "  adding: result/bnnmnist_2_lime_original_2.png (deflated 20%)\n",
            "  adding: result/bnnmnist_2_lime_original_0.png (deflated 23%)\n",
            "  adding: result/bnnmnist_3_lime_explain_2.png (deflated 20%)\n",
            "  adding: result/bnnmnist_8_lime_explain_0.png (deflated 20%)\n",
            "  adding: result/bnnmnist_5_lime_original_1.png (deflated 20%)\n",
            "  adding: result/bnnmnist_7_lime_explain_2.png (deflated 22%)\n",
            "  adding: result/bnnmnist_0_lime_original_1.png (deflated 20%)\n",
            "  adding: result/bnnmnist_8_lime_original_0.png (deflated 19%)\n",
            "  adding: result/bnnmnist_3_lime_original_2.png (deflated 20%)\n",
            "  adding: result/bnnmnist_6_lime_original_0.png (deflated 20%)\n",
            "  adding: result/bnnmnist_1_lime_original_0.png (deflated 23%)\n",
            "  adding: result/bnnmnist_0_lime_explain_1.png (deflated 21%)\n",
            "  adding: result/bnnmnist_8_lime_original_1.png (deflated 20%)\n",
            "  adding: result/bnnmnist_4_lime_original_0.png (deflated 20%)\n",
            "  adding: result/bnnmnist_9_lime_original_2.png (deflated 20%)\n",
            "  adding: result/bnnmnist_7_lime_original_2.png (deflated 21%)\n",
            "  adding: result/bnnmnist_1_lime_explain_0.png (deflated 24%)\n",
            "  adding: result/bnnmnist_4_lime_explain_0.png (deflated 23%)\n",
            "  adding: result/bnnmnist_9_lime_original_1.png (deflated 20%)\n",
            "  adding: result/bnnmnist_7_lime_original_0.png (deflated 20%)\n",
            "  adding: result/bnnmnist_3_lime_explain_0.png (deflated 20%)\n",
            "  adding: result/bnnmnist_6_lime_original_1.png (deflated 20%)\n",
            "  adding: result/bnnmnist_5_lime_explain_0.png (deflated 21%)\n",
            "  adding: result/bnnmnist_2_lime_explain_0.png (deflated 21%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqprZc54wG5N",
        "outputId": "f426ec7c-6811-4960-9d1c-ed070061aac5"
      },
      "source": [
        "files.download('bnnmnist_lime.zip')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_720ab0e7-e095-4da3-8d92-8b3e311135b3\", \"bnnmnist_lime.zip\", 239338)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gm562MvBSBY"
      },
      "source": [
        "def only_keep_important(img, mask, exp, ratio=0.2):\n",
        "  m, n = np.shape(mask)\n",
        "  N = int((1-ratio) * m * n)\n",
        "  imgg = img.clone()\n",
        "  # print(np.shape(imgg))\n",
        "  exp = sorted(exp, key=lambda x: x[1], reverse=True)\n",
        "  # print(exp,len(exp))\n",
        "  for idx in range(len(exp)):\n",
        "    i,j = exp[idx]\n",
        "    if j>= 0:\n",
        "      mask_ = mask.copy()\n",
        "      \n",
        "      k = len(mask_[mask_ == i])\n",
        "      mask_[mask_ == i] = 0\n",
        "      mask_[mask != i] = 1\n",
        "      # print(np.shape(mask_),np.shape(imgg))\n",
        "      # mask_ = np.concatenate([mask_,mask_,mask_])\n",
        "      # mask_ = mask_.reshape(32,32,3)\n",
        "      imgg = imgg * mask_ \n",
        "      \n",
        "      # print(mask_)\n",
        "      # print(imgg)\n",
        "      # print(N,k,i)\n",
        "      if N <= k:\n",
        "        # print('breaking')\n",
        "        break\n",
        "      N -=k\n",
        "    \n",
        "  return imgg"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7o42RZNiMln",
        "outputId": "5c2ac9f4-effe-4f54-e7e8-e696d8c75518"
      },
      "source": [
        "print(np.shape(images))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10000, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9ZF7_iXpa1v"
      },
      "source": [
        "## New Masked Images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAEc0LjvBTmI"
      },
      "source": [
        "import time\n",
        "\n",
        "rs = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "res = []\n",
        "N = np.shape(images)[0]\n",
        "print(np.shape(images))\n",
        "for ratio in rs:\n",
        "  new_images = []\n",
        "  t = time.time()\n",
        "  for i in range(2000):\n",
        "    img = images[i,:,:]\n",
        "    mask, exp, img_boundry = explain(img, show=False)\n",
        "    new_img = only_keep_important(img, mask, exp,ratio=ratio)\n",
        "    new_images.append([new_img, labels[i]])\n",
        "  print(time.time()-t)\n",
        "\n",
        "  new_testloader = torch.utils.data.DataLoader(new_images, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # since we're not\\ training, we don't need to calculate the gradients for our outputs\n",
        "  with torch.no_grad():\n",
        "      for data in new_testloader:\n",
        "          x, y = data\n",
        "          x = x.to(DEVICE).type(torch.FloatTensor)\n",
        "          y = y.to(DEVICE).type(torch.FloatTensor)\n",
        "          # images = images.to(DEVICE).type(torch.cuda.FloatTensor)\n",
        "          # labels = labels.to(DEVICE).type(torch.cuda.FloatTensor)\n",
        "          # calculate outputs by running images through the network\n",
        "          bs = x.shape[0]\n",
        "          x=  x[:,:,:].reshape(bs,-1)\n",
        "          outputs = model(x)\n",
        "          # the class with the highest energy is what we choose as prediction\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += y.size(0)\n",
        "          correct += (predicted == y).sum().item()\n",
        "  \n",
        "  res.append(100 * correct / total)\n",
        "\n",
        "for i in range(len(rs)):\n",
        "  print('ratio', rs[i])\n",
        "  print('Accuracy of the network on the test images: %d %%' % (res[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc3kl5_BrNAO"
      },
      "source": [
        "len(res)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nKZn4ZEketS"
      },
      "source": [
        "for i in range(len(rs)):\n",
        "  print('ratio', rs[i])\n",
        "  print('Accuracy of the network on the test images:', res[i])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQPDq-k_jZEP"
      },
      "source": [
        "print(len(new_images))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKNenqxeBWlJ"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWfaOkpIwoNj"
      },
      "source": [
        "# Latex Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT3kaU56wohD"
      },
      "source": [
        "for k in range(5):\n",
        "  print('\\\\begin{figure}[htbp]')\n",
        "  print('\\\\centering')\n",
        "  for i in range(2):\n",
        "    index = 2*k+i\n",
        "    for j in range(3):\n",
        "      print('\\\\begin{minipage}[t]{0.4\\\\textwidth}')\n",
        "      print('\\\\centering')\n",
        "      print('\\\\includegraphics[width=0.9\\\\linewidth]{bnnmnist_lime/bnnmnist_'+str(index)+'_lime_original_'+ str(j) +'.png}')\n",
        "      print('\\\\subcaption{Original images}')\n",
        "      # \\source{Autoria própria.}\n",
        "      print('\\\\end{minipage}')\n",
        "      print('\\\\hfill')\n",
        "      print('\\\\begin{minipage}[t]{0.4\\\\textwidth}')\n",
        "      print('\\\\centering')\n",
        "      print('\\\\includegraphics[width=0.9\\\\linewidth]{bnnmnist_lime/bnnmnist_'+str(index)+'_lime_explain_'+ str(j) +'.png}')\n",
        "      print('\\\\subcaption{Lime explanation of the image}')\n",
        "      print('\\\\end{minipage}')\n",
        "\n",
        "  print('\\\\caption{Original and Lime explanation of MNIST images}')\n",
        "  print('\\\\end{figure}')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pWZ8tS093NH"
      },
      "source": [
        "# Model Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAniQjyL-ALm"
      },
      "source": [
        "!pip install hiddenlayer"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncGDtgGh98H-"
      },
      "source": [
        "import hiddenlayer as hl\n",
        "\n",
        "transforms = [ hl.transforms.Prune('Constant') ] # Removes Constant nodes from graph.\n",
        "\n",
        "graph = hl.build_graph(model, torch.tensor(np.random.rand(10,28*28)).type(torch.FloatTensor), transforms=transforms)\n",
        "graph.theme = hl.graph.THEMES['blue'].copy()\n",
        "graph.save('BNN', format='png')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT7ro6j8-3QO"
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa4SxPYW-3e_"
      },
      "source": [
        "from torchviz import make_dot\n",
        "\n",
        "make_dot(model(torch.tensor(np.random.rand(128,28*28)).type(torch.FloatTensor)), params=dict(list(model.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")"
      ],
      "execution_count": 35,
      "outputs": []
    }
  ]
}