{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CiFAR10_Lime.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ton_nCYGVny"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lj3L_Eigjy6",
        "outputId": "b11e3968-63da-4271-fb44-c2c8c18dba73"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAvYAO1rHgS7"
      },
      "source": [
        "EPOCHS = 20\n",
        "DEVICE = 'cuda'\n",
        "batch_size = 2048"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDMA1LZ1TpvW"
      },
      "source": [
        "# CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15EoOkU9sYRh"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "504aa641b6ca4437a24a6dfe32cc87bf"
          ]
        },
        "id": "4W4hH8cxHhpL",
        "outputId": "3876a8c0-facc-41c7-85bb-65121a80a37d"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "504aa641b6ca4437a24a6dfe32cc87bf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCeI41qsexhs"
      },
      "source": [
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzKF2dWJjeD8"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfY_nVOSHjnz"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \"\"\"CNN.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"CNN Builder.\"\"\"\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv_layer = nn.Sequential(\n",
        "\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout2d(p=0.05),\n",
        "\n",
        "            # Conv Layer block 3\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(4096, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform forward.\"\"\"\n",
        "\n",
        "        # conv layers\n",
        "        x = self.conv_layer(x)\n",
        "\n",
        "        # flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # fc layer\n",
        "        x = self.fc_layer(x)\n",
        "\n",
        "        return x\n",
        "net = CNN()\n",
        "net = net.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSDYL_lOHlRL"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxK6YlZ_WjQm"
      },
      "source": [
        "## Train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ru0U7g5Hmm8"
      },
      "source": [
        "net.train()\n",
        "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
        "    batch_len = len(trainloader)\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "            \n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % int(batch_len/4) == int(batch_len/4) - 1:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / int(batch_len/4)))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9hw0BIkHoXE",
        "outputId": "74b5ce11-40dd-47b9-eb4d-3c9f88f42f45"
      },
      "source": [
        "net.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # print(images.shape, labels.shape)\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 82 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8PQrA2vWoQ-"
      },
      "source": [
        "## Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFMYwbDugKwD",
        "outputId": "f1538fa8-9896-44fb-8b3a-3d325d882fd6"
      },
      "source": [
        "torch.save(net.state_dict(), './drive/My Drive/saved_models/CIFAR10_model')\n",
        "print('saved')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_nGFjegWpl9"
      },
      "source": [
        "## Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVvjXliUWsPt",
        "outputId": "b6eab966-9aab-4773-879e-fbea24b39742"
      },
      "source": [
        "net.load_state_dict(torch.load('./drive/My Drive/saved_models/CIFAR10_model'))\n",
        "net.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv_layer): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (10): ReLU(inplace=True)\n",
              "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (12): Dropout2d(p=0.05, inplace=False)\n",
              "    (13): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc_layer): Sequential(\n",
              "    (0): Dropout(p=0.1, inplace=False)\n",
              "    (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.1, inplace=False)\n",
              "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gio9u2EnJ1u0"
      },
      "source": [
        "# LIME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr0rtdHwFOPM"
      },
      "source": [
        "import sys\n",
        "import inspect\n",
        "def has_arg(fn, arg_name):\n",
        "    \"\"\"Checks if a callable accepts a given keyword argument.\n",
        "    Args:\n",
        "        fn: callable to inspect\n",
        "        arg_name: string, keyword argument name to check\n",
        "    Returns:\n",
        "        bool, whether `fn` accepts a `arg_name` keyword argument.\n",
        "    \"\"\"\n",
        "    if sys.version_info < (3,):\n",
        "        if isinstance(fn, types.FunctionType) or isinstance(fn, types.MethodType):\n",
        "            arg_spec = inspect.getargspec(fn)\n",
        "        else:\n",
        "            try:\n",
        "                arg_spec = inspect.getargspec(fn.__call__)\n",
        "            except AttributeError:\n",
        "                return False\n",
        "        return (arg_name in arg_spec.args)\n",
        "    elif sys.version_info < (3, 6):\n",
        "        arg_spec = inspect.getfullargspec(fn)\n",
        "        return (arg_name in arg_spec.args or\n",
        "                arg_name in arg_spec.kwonlyargs)\n",
        "    else:\n",
        "        try:\n",
        "            signature = inspect.signature(fn)\n",
        "        except ValueError:\n",
        "            # handling Cython\n",
        "            signature = inspect.signature(fn.__call__)\n",
        "        parameter = signature.parameters.get(arg_name)\n",
        "        if parameter is None:\n",
        "            return False\n",
        "        return (parameter.kind in (inspect.Parameter.POSITIONAL_OR_KEYWORD,\n",
        "                                   inspect.Parameter.KEYWORD_ONLY))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilOGO0t-J22r"
      },
      "source": [
        "import types\n",
        "\n",
        "from skimage.segmentation import felzenszwalb, slic, quickshift\n",
        "\n",
        "\n",
        "class BaseWrapper(object):\n",
        "    \"\"\"Base class for LIME Scikit-Image wrapper\n",
        "\n",
        "\n",
        "    Args:\n",
        "        target_fn: callable function or class instance\n",
        "        target_params: dict, parameters to pass to the target_fn\n",
        "\n",
        "\n",
        "    'target_params' takes parameters required to instanciate the\n",
        "        desired Scikit-Image class/model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, target_fn=None, **target_params):\n",
        "        self.target_fn = target_fn\n",
        "        self.target_params = target_params\n",
        "\n",
        "    def _check_params(self, parameters):\n",
        "        \"\"\"Checks for mistakes in 'parameters'\n",
        "\n",
        "        Args :\n",
        "            parameters: dict, parameters to be checked\n",
        "\n",
        "        Raises :\n",
        "            ValueError: if any parameter is not a valid argument for the target function\n",
        "                or the target function is not defined\n",
        "            TypeError: if argument parameters is not iterable\n",
        "         \"\"\"\n",
        "        a_valid_fn = []\n",
        "        if self.target_fn is None:\n",
        "            if callable(self):\n",
        "                a_valid_fn.append(self.__call__)\n",
        "            else:\n",
        "                raise TypeError('invalid argument: tested object is not callable,\\\n",
        "                 please provide a valid target_fn')\n",
        "        elif isinstance(self.target_fn, types.FunctionType) \\\n",
        "                or isinstance(self.target_fn, types.MethodType):\n",
        "            a_valid_fn.append(self.target_fn)\n",
        "        else:\n",
        "            a_valid_fn.append(self.target_fn.__call__)\n",
        "\n",
        "        if not isinstance(parameters, str):\n",
        "            for p in parameters:\n",
        "                for fn in a_valid_fn:\n",
        "                    if has_arg(fn, p):\n",
        "                        pass\n",
        "                    else:\n",
        "                        raise ValueError('{} is not a valid parameter'.format(p))\n",
        "        else:\n",
        "            raise TypeError('invalid argument: list or dictionnary expected')\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        \"\"\"Sets the parameters of this estimator.\n",
        "        Args:\n",
        "            **params: Dictionary of parameter names mapped to their values.\n",
        "\n",
        "        Raises :\n",
        "            ValueError: if any parameter is not a valid argument\n",
        "                for the target function\n",
        "        \"\"\"\n",
        "        self._check_params(params)\n",
        "        self.target_params = params\n",
        "\n",
        "    def filter_params(self, fn, override=None):\n",
        "        \"\"\"Filters `target_params` and return those in `fn`'s arguments.\n",
        "        Args:\n",
        "            fn : arbitrary function\n",
        "            override: dict, values to override target_params\n",
        "        Returns:\n",
        "            result : dict, dictionary containing variables\n",
        "            in both target_params and fn's arguments.\n",
        "        \"\"\"\n",
        "        override = override or {}\n",
        "        result = {}\n",
        "        for name, value in self.target_params.items():\n",
        "            if has_arg(fn, name):\n",
        "                result.update({name: value})\n",
        "        result.update(override)\n",
        "        return result\n",
        "\n",
        "\n",
        "class SegmentationAlgorithm(BaseWrapper):\n",
        "    \"\"\" Define the image segmentation function based on Scikit-Image\n",
        "            implementation and a set of provided parameters\n",
        "\n",
        "        Args:\n",
        "            algo_type: string, segmentation algorithm among the following:\n",
        "                'quickshift', 'slic', 'felzenszwalb'\n",
        "            target_params: dict, algorithm parameters (valid model paramters\n",
        "                as define in Scikit-Image documentation)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, algo_type, **target_params):\n",
        "        self.algo_type = algo_type\n",
        "        if (self.algo_type == 'quickshift'):\n",
        "            BaseWrapper.__init__(self, quickshift, **target_params)\n",
        "            kwargs = self.filter_params(quickshift)\n",
        "            self.set_params(**kwargs)\n",
        "        elif (self.algo_type == 'felzenszwalb'):\n",
        "            BaseWrapper.__init__(self, felzenszwalb, **target_params)\n",
        "            kwargs = self.filter_params(felzenszwalb)\n",
        "            self.set_params(**kwargs)\n",
        "        elif (self.algo_type == 'slic'):\n",
        "            BaseWrapper.__init__(self, slic, **target_params)\n",
        "            kwargs = self.filter_params(slic)\n",
        "            self.set_params(**kwargs)\n",
        "\n",
        "    def __call__(self, *args):\n",
        "        return self.target_fn(args[0], **self.target_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bfx_ju-Tw0r"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn.linear_model import Ridge, lars_path\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "\n",
        "class LimeBase(object):\n",
        "    \"\"\"Class for learning a locally linear sparse model from perturbed data\"\"\"\n",
        "    def __init__(self,\n",
        "                 kernel_fn,\n",
        "                 verbose=False,\n",
        "                 random_state=None):\n",
        "\n",
        "        self.kernel_fn = kernel_fn\n",
        "        self.verbose = verbose\n",
        "        self.random_state = check_random_state(random_state)\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_lars_path(weighted_data, weighted_labels):\n",
        "\n",
        "        x_vector = weighted_data\n",
        "        alphas, _, coefs = lars_path(x_vector,\n",
        "                                     weighted_labels,\n",
        "                                     method='lasso',\n",
        "                                     verbose=False)\n",
        "        return alphas, coefs\n",
        "\n",
        "    def forward_selection(self, data, labels, weights, num_features):\n",
        "        \"\"\"Iteratively adds features to the model\"\"\"\n",
        "        clf = Ridge(alpha=0, fit_intercept=True, random_state=self.random_state)\n",
        "        used_features = []\n",
        "        for _ in range(min(num_features, data.shape[1])):\n",
        "            max_ = -100000000\n",
        "            best = 0\n",
        "            for feature in range(data.shape[1]):\n",
        "                if feature in used_features:\n",
        "                    continue\n",
        "                clf.fit(data[:, used_features + [feature]], labels,\n",
        "                        sample_weight=weights)\n",
        "                score = clf.score(data[:, used_features + [feature]],\n",
        "                                  labels,\n",
        "                                  sample_weight=weights)\n",
        "                if score > max_:\n",
        "                    best = feature\n",
        "                    max_ = score\n",
        "            used_features.append(best)\n",
        "        return np.array(used_features)\n",
        "\n",
        "    def feature_selection(self, data, labels, weights, num_features, method):\n",
        "        if method == 'none':\n",
        "            return np.array(range(data.shape[1]))\n",
        "        elif method == 'forward_selection':\n",
        "            return self.forward_selection(data, labels, weights, num_features)\n",
        "        elif method == 'highest_weights':\n",
        "            clf = Ridge(alpha=0.01, fit_intercept=True,\n",
        "                        random_state=self.random_state)\n",
        "            clf.fit(data, labels, sample_weight=weights)\n",
        "\n",
        "            coef = clf.coef_\n",
        "            if sp.sparse.issparse(data):\n",
        "                coef = sp.sparse.csr_matrix(clf.coef_)\n",
        "                weighted_data = coef.multiply(data[0])\n",
        "                # Note: most efficient to slice the data before reversing\n",
        "                sdata = len(weighted_data.data)\n",
        "                argsort_data = np.abs(weighted_data.data).argsort()\n",
        "                # Edge case where data is more sparse than requested number of feature importances\n",
        "                # In that case, we just pad with zero-valued features\n",
        "                if sdata < num_features:\n",
        "                    nnz_indexes = argsort_data[::-1]\n",
        "                    indices = weighted_data.indices[nnz_indexes]\n",
        "                    num_to_pad = num_features - sdata\n",
        "                    indices = np.concatenate((indices, np.zeros(num_to_pad, dtype=indices.dtype)))\n",
        "                    indices_set = set(indices)\n",
        "                    pad_counter = 0\n",
        "                    for i in range(data.shape[1]):\n",
        "                        if i not in indices_set:\n",
        "                            indices[pad_counter + sdata] = i\n",
        "                            pad_counter += 1\n",
        "                            if pad_counter >= num_to_pad:\n",
        "                                break\n",
        "                else:\n",
        "                    nnz_indexes = argsort_data[sdata - num_features:sdata][::-1]\n",
        "                    indices = weighted_data.indices[nnz_indexes]\n",
        "                return indices\n",
        "            else:\n",
        "                weighted_data = coef * data[0]\n",
        "                feature_weights = sorted(\n",
        "                    zip(range(data.shape[1]), weighted_data),\n",
        "                    key=lambda x: np.abs(x[1]),\n",
        "                    reverse=True)\n",
        "                return np.array([x[0] for x in feature_weights[:num_features]])\n",
        "        elif method == 'lasso_path':\n",
        "            weighted_data = ((data - np.average(data, axis=0, weights=weights))\n",
        "                             * np.sqrt(weights[:, np.newaxis]))\n",
        "            weighted_labels = ((labels - np.average(labels, weights=weights))\n",
        "                               * np.sqrt(weights))\n",
        "            nonzero = range(weighted_data.shape[1])\n",
        "            _, coefs = self.generate_lars_path(weighted_data,\n",
        "                                               weighted_labels)\n",
        "            for i in range(len(coefs.T) - 1, 0, -1):\n",
        "                nonzero = coefs.T[i].nonzero()[0]\n",
        "                if len(nonzero) <= num_features:\n",
        "                    break\n",
        "            used_features = nonzero\n",
        "            return used_features\n",
        "        elif method == 'auto':\n",
        "            if num_features <= 6:\n",
        "                n_method = 'forward_selection'\n",
        "            else:\n",
        "                n_method = 'highest_weights'\n",
        "            return self.feature_selection(data, labels, weights,\n",
        "                                          num_features, n_method)\n",
        "\n",
        "    def explain_instance_with_data(self,\n",
        "                                   neighborhood_data,\n",
        "                                   neighborhood_labels,\n",
        "                                   distances,\n",
        "                                   label,\n",
        "                                   num_features,\n",
        "                                   feature_selection='auto',\n",
        "                                   model_regressor=None):\n",
        "      \n",
        "        weights = self.kernel_fn(distances)\n",
        "        labels_column = neighborhood_labels[:, label]\n",
        "        used_features = self.feature_selection(neighborhood_data,\n",
        "                                               labels_column,\n",
        "                                               weights,\n",
        "                                               num_features,\n",
        "                                               feature_selection)\n",
        "        if model_regressor is None:\n",
        "            model_regressor = Ridge(alpha=1, fit_intercept=True,\n",
        "                                    random_state=self.random_state)\n",
        "        easy_model = model_regressor\n",
        "        easy_model.fit(neighborhood_data[:, used_features],\n",
        "                       labels_column, sample_weight=weights)\n",
        "        prediction_score = easy_model.score(\n",
        "            neighborhood_data[:, used_features],\n",
        "            labels_column, sample_weight=weights)\n",
        "\n",
        "        local_pred = easy_model.predict(neighborhood_data[0, used_features].reshape(1, -1))\n",
        "\n",
        "        if self.verbose:\n",
        "            print('Intercept', easy_model.intercept_)\n",
        "            print('Prediction_local', local_pred,)\n",
        "            print('Right:', neighborhood_labels[0, label])\n",
        "        return (easy_model.intercept_,\n",
        "                sorted(zip(used_features, easy_model.coef_),\n",
        "                       key=lambda x: np.abs(x[1]), reverse=True),\n",
        "                prediction_score, local_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKuLosxMTyfU"
      },
      "source": [
        "import copy\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.utils import check_random_state\n",
        "from skimage.color import gray2rgb\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ImageExplanation(object):\n",
        "    def __init__(self, image, segments):\n",
        "        self.image = image\n",
        "        self.segments = segments\n",
        "        self.intercept = {}\n",
        "        self.local_exp = {}\n",
        "        self.local_pred = {}\n",
        "        self.score = {}\n",
        "\n",
        "    def get_image_and_mask(self, label, positive_only=True, negative_only=False, hide_rest=False,\n",
        "                           num_features=5, min_weight=0.):\n",
        "        if label not in self.local_exp:\n",
        "            raise KeyError('Label not in explanation')\n",
        "        if positive_only & negative_only:\n",
        "            raise ValueError(\"Positive_only and negative_only cannot be true at the same time.\")\n",
        "        segments = self.segments\n",
        "        image = self.image\n",
        "        exp = self.local_exp[label]\n",
        "        mask = np.zeros(segments.shape, segments.dtype)\n",
        "        if hide_rest:\n",
        "            temp = np.zeros(self.image.shape)\n",
        "        else:\n",
        "            temp = self.image.copy()\n",
        "        if positive_only:\n",
        "            fs = [x[0] for x in exp\n",
        "                  if x[1] > 0 and x[1] > min_weight][:num_features]\n",
        "        if negative_only:\n",
        "            fs = [x[0] for x in exp\n",
        "                  if x[1] < 0 and abs(x[1]) > min_weight][:num_features]\n",
        "        if positive_only or negative_only:\n",
        "            for f in fs:\n",
        "                temp[segments == f] = image[segments == f].copy()\n",
        "                mask[segments == f] = 1\n",
        "            return temp, mask\n",
        "        else:\n",
        "            for f, w in exp[:num_features]:\n",
        "                if np.abs(w) < min_weight:\n",
        "                    continue\n",
        "                c = 0 if w < 0 else 1\n",
        "                mask[segments == f] = -1 if w < 0 else 1\n",
        "                temp[segments == f] = image[segments == f].copy()\n",
        "                temp[segments == f, c] = np.max(image)\n",
        "            return temp, mask\n",
        "\n",
        "\n",
        "class LimeImageExplainer(object):\n",
        "    def __init__(self, kernel_width=.25, kernel=None, verbose=False,\n",
        "                 feature_selection='auto', random_state=None):\n",
        "        kernel_width = float(kernel_width)\n",
        "\n",
        "        if kernel is None:\n",
        "            def kernel(d, kernel_width):\n",
        "                return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
        "\n",
        "        kernel_fn = partial(kernel, kernel_width=kernel_width)\n",
        "\n",
        "        self.random_state = check_random_state(random_state)\n",
        "        self.feature_selection = feature_selection\n",
        "        self.base = LimeBase(kernel_fn, verbose, random_state=self.random_state)\n",
        "\n",
        "    def explain_instance(self, image, classifier_fn, labels=(1,),\n",
        "                         hide_color=None,\n",
        "                         top_labels=5, num_features=100000, num_samples=1000,\n",
        "                         batch_size=10,\n",
        "                         segmentation_fn=None,\n",
        "                         distance_metric='cosine',\n",
        "                         model_regressor=None,\n",
        "                         random_seed=None,\n",
        "                         progress_bar=True):\n",
        "        \n",
        "        if len(image.shape) == 2:\n",
        "            image = gray2rgb(image)\n",
        "        if random_seed is None:\n",
        "            random_seed = self.random_state.randint(0, high=1000)\n",
        "\n",
        "        if segmentation_fn is None:\n",
        "            segmentation_fn = SegmentationAlgorithm('quickshift', kernel_size=4,\n",
        "                                                    max_dist=200, ratio=0.2,\n",
        "                                                    random_seed=random_seed)\n",
        "        segments = segmentation_fn(image)\n",
        "\n",
        "        fudged_image = image.copy()\n",
        "        if hide_color is None:\n",
        "            for x in np.unique(segments):\n",
        "                fudged_image[segments == x] = (\n",
        "                    np.mean(image[segments == x][:, 0]),\n",
        "                    np.mean(image[segments == x][:, 1]),\n",
        "                    np.mean(image[segments == x][:, 2]))\n",
        "        else:\n",
        "            fudged_image[:] = hide_color\n",
        "\n",
        "        top = labels\n",
        "\n",
        "        data, labels = self.data_labels(image, fudged_image, segments,\n",
        "                                        classifier_fn, num_samples,\n",
        "                                        batch_size=batch_size,\n",
        "                                        progress_bar=progress_bar)\n",
        "\n",
        "        distances = sklearn.metrics.pairwise_distances(\n",
        "            data,\n",
        "            data[0].reshape(1, -1),\n",
        "            metric=distance_metric\n",
        "        ).ravel()\n",
        "\n",
        "        ret_exp = ImageExplanation(image, segments)\n",
        "        if top_labels:\n",
        "            top = np.argsort(labels[0])[-top_labels:]\n",
        "            ret_exp.top_labels = list(top)\n",
        "            ret_exp.top_labels.reverse()\n",
        "        for label in top:\n",
        "            (ret_exp.intercept[label],\n",
        "             ret_exp.local_exp[label],\n",
        "             ret_exp.score[label],\n",
        "             ret_exp.local_pred[label]) = self.base.explain_instance_with_data(\n",
        "                data, labels, distances, label, num_features,\n",
        "                model_regressor=model_regressor,\n",
        "                feature_selection=self.feature_selection)\n",
        "        return ret_exp\n",
        "\n",
        "    def data_labels(self,\n",
        "                    image,\n",
        "                    fudged_image,\n",
        "                    segments,\n",
        "                    classifier_fn,\n",
        "                    num_samples,\n",
        "                    batch_size=10,\n",
        "                    progress_bar=True):\n",
        "    \n",
        "        n_features = np.unique(segments).shape[0]\n",
        "        data = self.random_state.randint(0, 2, num_samples * n_features)\\\n",
        "            .reshape((num_samples, n_features))\n",
        "        labels = []\n",
        "        data[0, :] = 1\n",
        "        imgs = []\n",
        "        rows = tqdm(data) if progress_bar else data\n",
        "        for row in rows:\n",
        "            temp = copy.deepcopy(image)\n",
        "            zeros = np.where(row == 0)[0]\n",
        "            mask = np.zeros(segments.shape).astype(bool)\n",
        "            for z in zeros:\n",
        "                mask[segments == z] = True\n",
        "            temp[mask] = fudged_image[mask]\n",
        "            imgs.append(temp)\n",
        "            if len(imgs) == batch_size:\n",
        "                preds = classifier_fn(np.array(imgs))\n",
        "                labels.extend(preds)\n",
        "                imgs = []\n",
        "        if len(imgs) > 0:\n",
        "            preds = classifier_fn(np.array(imgs))\n",
        "            labels.extend(preds)\n",
        "        return data, np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5SU5h8eWGK0"
      },
      "source": [
        "def get_pil_transform(): \n",
        "    transf = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.CenterCrop(224)\n",
        "    ])    \n",
        "    return transf\n",
        "\n",
        "def get_preprocess_transform():\n",
        "    normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))     \n",
        "    transf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])    \n",
        "\n",
        "    return transf    \n",
        "\n",
        "pill_transf = get_pil_transform()\n",
        "preprocess_transform = get_preprocess_transform()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oeVsZxvT039"
      },
      "source": [
        "# Explaination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIoTJcTC6heE"
      },
      "source": [
        "def batch_predict(images):\n",
        "    net.eval()\n",
        "    batch = torch.stack(tuple(preprocess_transform(i) for i in images), dim=0)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    net.to(device)\n",
        "    batch = batch.to(device)\n",
        "\n",
        "    logits = net(batch)\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    return probs.detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JzrWhsq6bn9"
      },
      "source": [
        "from skimage.segmentation import mark_boundaries\n",
        "import random\n",
        "segmentation_fn = SegmentationAlgorithm('quickshift', kernel_size=2,\n",
        "                                          max_dist=3, ratio=0.2,\n",
        "                                          random_seed= int(random.random()*1000))\n",
        "def explain(img,show=False):\n",
        "  if show:\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "  explainer = LimeImageExplainer()\n",
        "  explanation = explainer.explain_instance(img, \n",
        "                                         batch_predict, # classification function\n",
        "                                         top_labels=1, \n",
        "                                        #  hide_color=0, \n",
        "                                        segmentation_fn = segmentation_fn,\n",
        "                                         num_samples=100 )\n",
        "\n",
        "  temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only= False, num_features=20, hide_rest=False)\n",
        "  img_boundry1 = mark_boundaries(temp/255.0, mask, color=(1, 1, 0))\n",
        "  if show:\n",
        "  \n",
        "    # print(explanation.local_exp[explanation.top_labels[0]])\n",
        "    # print(explanation.segments)\n",
        "    plt.imshow(img_boundry1)\n",
        "    plt.show()\n",
        "    plt.imshow(mask)\n",
        "    plt.show()\n",
        "  return img_boundry1, explanation.segments, explanation.local_exp[explanation.top_labels[0]], explanation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdzj093KPq-B"
      },
      "source": [
        "def only_keep_important(img, mask, exp, ratio=0.2):\n",
        "  m, n = np.shape(mask)\n",
        "  N = int((1-ratio) * m * n)\n",
        "  imgg = img.copy()\n",
        "  # print(np.shape(imgg))\n",
        "  exp = sorted(exp, key=lambda x: x[1], reverse=True)\n",
        "  # print(exp,len(exp))\n",
        "  for idx in range(len(exp)):\n",
        "    i,j = exp[idx]\n",
        "    if j>= 0:\n",
        "      mask_ = mask.copy()\n",
        "      \n",
        "      k = len(mask_[mask_ == i])\n",
        "      mask_[mask_ == i] = 0\n",
        "      mask_[mask != i] = 1\n",
        "      # print(np.shape(mask_),np.shape(imgg))\n",
        "      mask_ = np.concatenate([mask_,mask_,mask_])\n",
        "      mask_ = mask_.reshape(32,32,3)\n",
        "      imgg = imgg * mask_ \n",
        "      \n",
        "      # print(mask_)\n",
        "      # print(imgg)\n",
        "      # print(N,k,i)\n",
        "      if N <= k:\n",
        "        # print('breaking')\n",
        "        break\n",
        "      N -=k\n",
        "    \n",
        "  return imgg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ6hnq88T3Ey"
      },
      "source": [
        "import random\n",
        "from google.colab import files\n",
        "images =  testset.data\n",
        "labels =  testset.targets\n",
        "N = np.shape(images)[0]\n",
        "samples = []\n",
        "indexes = []\n",
        "for i in range(10):\n",
        "  a =  [j for j in range(N) if labels[j]==i]\n",
        "  random.shuffle(a) \n",
        "  indexes.extend(a[:3])\n",
        "\n",
        "# indexes = np.random.permutation(np.shape(images)[0])\n",
        "# print(len(indexes))\n",
        "import os\n",
        "os.mkdir('result')\n",
        "for i in range(len(indexes)):\n",
        "  img = images[indexes[i],:,:,:]\n",
        "  img_boundry, segs, epx, explanation = explain(img, show=False)\n",
        "  \n",
        "  plt.imshow(img)\n",
        "  label = classes[int(i/3)]\n",
        "  plt.title('GT: ' + str(label) + ', Pred: ' + classes[explanation.top_levels[0]])\n",
        "  plt.savefig('result/cifar10_'+str(label)+'_lime_original_'+str(i%3)+'.png')\n",
        "  # files.download('result/cifar10_'+str(label)+'_lime_original_'+str(i%3)+'.png')\n",
        "  plt.show()\n",
        "  plt.imshow(img_boundry)\n",
        "  plt.savefig('result/cifar10_'+str(label)+'_lime_explain_'+str(i%3)+'.png')\n",
        "  # files.download('result/cifar10_'+str(label)+'_lime_explain_'+str(i%3)+'.png')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L65_qm1fYo-c",
        "outputId": "ab528fd0-32f0-4f7c-e3df-28a119225f83"
      },
      "source": [
        "! zip -r cifar10_lime.zip result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: result/ (stored 0%)\n",
            "  adding: result/cifar10_horse_lime_explain_1.png (deflated 12%)\n",
            "  adding: result/cifar10_deer_lime_explain_2.png (deflated 12%)\n",
            "  adding: result/cifar10_frog_lime_original_2.png (deflated 11%)\n",
            "  adding: result/cifar10_car_lime_explain_2.png (deflated 12%)\n",
            "  adding: result/cifar10_plane_lime_explain_1.png (deflated 13%)\n",
            "  adding: result/cifar10_plane_lime_explain_2.png (deflated 13%)\n",
            "  adding: result/cifar10_horse_lime_explain_0.png (deflated 12%)\n",
            "  adding: result/cifar10_deer_lime_original_2.png (deflated 12%)\n",
            "  adding: result/cifar10_plane_lime_explain_0.png (deflated 13%)\n",
            "  adding: result/cifar10_car_lime_original_0.png (deflated 11%)\n",
            "  adding: result/cifar10_cat_lime_original_1.png (deflated 11%)\n",
            "  adding: result/cifar10_frog_lime_explain_2.png (deflated 12%)\n",
            "  adding: result/cifar10_bird_lime_explain_1.png (deflated 13%)\n",
            "  adding: result/cifar10_horse_lime_original_2.png (deflated 11%)\n",
            "  adding: result/cifar10_plane_lime_original_1.png (deflated 11%)\n",
            "  adding: result/cifar10_cat_lime_original_2.png (deflated 11%)\n",
            "  adding: result/cifar10_truck_lime_original_2.png (deflated 11%)\n",
            "  adding: result/cifar10_ship_lime_explain_1.png (deflated 13%)\n",
            "  adding: result/cifar10_deer_lime_original_1.png (deflated 11%)\n",
            "  adding: result/cifar10_deer_lime_explain_0.png (deflated 11%)\n",
            "  adding: result/cifar10_horse_lime_original_0.png (deflated 11%)\n",
            "  adding: result/cifar10_frog_lime_original_1.png (deflated 11%)\n",
            "  adding: result/cifar10_truck_lime_original_0.png (deflated 12%)\n",
            "  adding: result/cifar10_dog_lime_explain_0.png (deflated 14%)\n",
            "  adding: result/cifar10_bird_lime_explain_0.png (deflated 13%)\n",
            "  adding: result/cifar10_plane_lime_original_0.png (deflated 11%)\n",
            "  adding: result/cifar10_horse_lime_original_1.png (deflated 11%)\n",
            "  adding: result/cifar10_bird_lime_original_2.png (deflated 11%)\n",
            "  adding: result/cifar10_plane_lime_original_2.png (deflated 12%)\n",
            "  adding: result/cifar10_horse_lime_explain_2.png (deflated 12%)\n",
            "  adding: result/cifar10_deer_lime_explain_1.png (deflated 13%)\n",
            "  adding: result/cifar10_truck_lime_explain_0.png (deflated 12%)\n",
            "  adding: result/cifar10_dog_lime_explain_1.png (deflated 11%)\n",
            "  adding: result/cifar10_truck_lime_explain_1.png (deflated 12%)\n",
            "  adding: result/cifar10_ship_lime_explain_2.png (deflated 13%)\n",
            "  adding: result/cifar10_dog_lime_original_0.png (deflated 15%)\n",
            "  adding: result/cifar10_cat_lime_explain_2.png (deflated 13%)\n",
            "  adding: result/cifar10_car_lime_explain_0.png (deflated 12%)\n",
            "  adding: result/cifar10_ship_lime_original_1.png (deflated 11%)\n",
            "  adding: result/cifar10_car_lime_original_2.png (deflated 11%)\n",
            "  adding: result/cifar10_frog_lime_explain_0.png (deflated 12%)\n",
            "  adding: result/cifar10_cat_lime_explain_0.png (deflated 12%)\n",
            "  adding: result/cifar10_cat_lime_explain_1.png (deflated 12%)\n",
            "  adding: result/cifar10_cat_lime_original_0.png (deflated 12%)\n",
            "  adding: result/cifar10_bird_lime_original_0.png (deflated 11%)\n",
            "  adding: result/cifar10_truck_lime_explain_2.png (deflated 12%)\n",
            "  adding: result/cifar10_dog_lime_original_1.png (deflated 13%)\n",
            "  adding: result/cifar10_ship_lime_original_2.png (deflated 11%)\n",
            "  adding: result/cifar10_frog_lime_explain_1.png (deflated 12%)\n",
            "  adding: result/cifar10_car_lime_explain_1.png (deflated 12%)\n",
            "  adding: result/cifar10_truck_lime_original_1.png (deflated 12%)\n",
            "  adding: result/cifar10_deer_lime_original_0.png (deflated 12%)\n",
            "  adding: result/cifar10_ship_lime_explain_0.png (deflated 12%)\n",
            "  adding: result/cifar10_dog_lime_explain_2.png (deflated 12%)\n",
            "  adding: result/cifar10_frog_lime_original_0.png (deflated 12%)\n",
            "  adding: result/cifar10_ship_lime_original_0.png (deflated 11%)\n",
            "  adding: result/cifar10_car_lime_original_1.png (deflated 11%)\n",
            "  adding: result/cifar10_dog_lime_original_2.png (deflated 11%)\n",
            "  adding: result/cifar10_bird_lime_original_1.png (deflated 11%)\n",
            "  adding: result/cifar10_bird_lime_explain_2.png (deflated 12%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "zGL3VLBWYt4s",
        "outputId": "081812f8-c23c-46ed-b9a3-591139781b02"
      },
      "source": [
        "files.download('cifar10_lime.zip')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_5d64347d-2816-46b8-ab81-bcb70b925301\", \"cifar10_lime.zip\", 485275)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce08jE16rtYh"
      },
      "source": [
        "import time\n",
        "N = np.shape(testset.data)[0]\n",
        "print(N)\n",
        "new_images = []\n",
        "t = time.time()\n",
        "for i in range(2000):\n",
        "  img = testset.data[i,:,:,:]\n",
        "  img_boundry ,mask, exp, exp_loc = explain(img,show=False)\n",
        "  new_img = only_keep_important(img, mask, exp)\n",
        "  new_images.append([new_img.reshape(3,32,32), testset.targets[i]])\n",
        "print(time.time()-t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMTzI22hlUXW"
      },
      "source": [
        "new_testloader = torch.utils.data.DataLoader(new_images, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJL258QqsWPA",
        "outputId": "31744c2d-c462-4dc3-b004-7e04ed3070c1"
      },
      "source": [
        "net.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not\\ training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in new_testloader:\n",
        "        images, labels = data\n",
        "        # print(images.shape, labels.shape)\n",
        "        images = images.to(DEVICE).type(torch.cuda.FloatTensor)\n",
        "        labels = labels.to(DEVICE).type(torch.cuda.FloatTensor)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print('Accuracy of the network on the test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 12 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8UKL9Jqlkh1",
        "outputId": "d09870f6-aa18-4a66-8f43-ae8c2872ca59"
      },
      "source": [
        "print(len(new_images))\n",
        "np.shape(testset.targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOn6Eluy8DD1"
      },
      "source": [
        "# Model Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idPPv9pH8KsW",
        "outputId": "56a06807-ed75-4188-d989-77d6b82a7e21"
      },
      "source": [
        "!pip install hiddenlayer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hiddenlayer\n",
            "  Downloading https://files.pythonhosted.org/packages/64/f8/ea51d02695a4dc397f3b2487fae462cd3f2ce707c54250e0fdfaec2ff92e/hiddenlayer-0.3-py3-none-any.whl\n",
            "Installing collected packages: hiddenlayer\n",
            "Successfully installed hiddenlayer-0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyvL5Uih8F6O",
        "outputId": "6b83d6a5-cc3d-43b6-9af9-e3b824d7b684"
      },
      "source": [
        "import hiddenlayer as hl\n",
        "\n",
        "transforms = [ hl.transforms.Prune('Constant') ] # Removes Constant nodes from graph.\n",
        "\n",
        "graph = hl.build_graph(net, torch.tensor(np.random.rand(10,3,32,32),device='cuda').type(torch.cuda.FloatTensor), transforms=transforms)\n",
        "graph.theme = hl.graph.THEMES['blue'].copy()\n",
        "graph.save('CIFAR10_Model', format='png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/onnx/symbolic_helper.py:715: UserWarning: ONNX export mode is set to inference mode, but operator batch_norm is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
            "  training_mode + \", as specified by the export mode.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/onnx/symbolic_helper.py:715: UserWarning: ONNX export mode is set to inference mode, but operator dropout is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
            "  training_mode + \", as specified by the export mode.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmS9fYsu9LPn",
        "outputId": "e6e7c5ba-f50f-402e-e676-37f5819b6570"
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "  Downloading https://files.pythonhosted.org/packages/79/e7/643808913211d6c1fc96a3a4333bf4c9276858fab00bcafaf98ea58a97be/torchviz-0.0.2.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.9.0+cu102)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.7.4.3)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-cp37-none-any.whl size=4152 sha256=5389c4c351e4e5c140604439143a0c66f4027fd4401f9b91e602fd59f0b9db3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/26/58/026ffd533dbe8b3972eb423da9c7949beca68d1c98ed9e8624\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "770223ml9Fds",
        "outputId": "4f1064d9-b889-4946-d58d-41153975e71b"
      },
      "source": [
        "from torchviz import make_dot\n",
        "\n",
        "make_dot(net(torch.tensor(np.random.rand(128,3,32,32),device='cuda').type(torch.cuda.FloatTensor)), params=dict(list(net.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'rnn_torchviz.png'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qQi_NzYd_mo"
      },
      "source": [
        "# Latex Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cdB9h6Hd_-d",
        "outputId": "88a7ea0d-7dcd-44ad-b6b2-6faa2acc25e4"
      },
      "source": [
        "for k in range(5):\n",
        "  print('\\\\begin{figure}[htbp]')\n",
        "  print('\\\\centering')\n",
        "  for i in range(2):\n",
        "    index = 2*k+i\n",
        "    for j in range(3):\n",
        "      print('\\\\begin{minipage}[t]{0.4\\\\textwidth}')\n",
        "      print('\\\\centering')\n",
        "      print('\\\\includegraphics[width=0.9\\\\linewidth]{cifar10_lime/cifar10_'+str(classes[index])+'_lime_original_'+ str(j) +'.png}')\n",
        "      # print('\\\\subcaption{Original images}')\n",
        "      # \\source{Autoria própria.}\n",
        "      print('\\\\end{minipage}')\n",
        "      print('\\\\hfill')\n",
        "      print('\\\\begin{minipage}[t]{0.4\\\\textwidth}')\n",
        "      print('\\\\centering')\n",
        "      print('\\\\includegraphics[width=0.9\\\\linewidth]{cifar10_lime/cifar10_'+str(classes[index])+'_lime_explain_'+ str(j) +'.png}')\n",
        "      # print('\\\\subcaption{LRP explanation of the image}')\n",
        "      print('\\\\end{minipage}')\n",
        "\n",
        "  print('\\\\caption{Original and Lime explanation of CIFAR10 images. Images in left column are original images and images in the right column are LIME explanations.}')\n",
        "  print('\\\\end{figure}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\\begin{figure}[htbp]\n",
            "\\centering\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_plane_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_plane_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_plane_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_plane_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_plane_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_plane_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_car_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_car_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_car_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_car_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_car_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_car_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\caption{Original and Lime explanation of CIFAR10 images. Images in left column are original images and images in the right column are LIME explanations.}\n",
            "\\end{figure}\n",
            "\\begin{figure}[htbp]\n",
            "\\centering\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_bird_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_bird_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_bird_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_bird_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_bird_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_bird_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_cat_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_cat_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_cat_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_cat_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_cat_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_cat_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\caption{Original and Lime explanation of CIFAR10 images. Images in left column are original images and images in the right column are LIME explanations.}\n",
            "\\end{figure}\n",
            "\\begin{figure}[htbp]\n",
            "\\centering\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_deer_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_deer_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_deer_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_deer_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_deer_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_deer_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_dog_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_dog_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_dog_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_dog_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_dog_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_dog_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\caption{Original and Lime explanation of CIFAR10 images. Images in left column are original images and images in the right column are LIME explanations.}\n",
            "\\end{figure}\n",
            "\\begin{figure}[htbp]\n",
            "\\centering\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_frog_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_frog_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_frog_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_frog_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_frog_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_frog_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_horse_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_horse_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_horse_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_horse_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_horse_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_horse_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\caption{Original and Lime explanation of CIFAR10 images. Images in left column are original images and images in the right column are LIME explanations.}\n",
            "\\end{figure}\n",
            "\\begin{figure}[htbp]\n",
            "\\centering\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_ship_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_ship_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_ship_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_ship_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_ship_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_ship_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_truck_lime_original_0.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_truck_lime_explain_0.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_truck_lime_original_1.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_truck_lime_explain_1.png}\n",
            "\\end{minipage}\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_truck_lime_original_2.png}\n",
            "\\end{minipage}\n",
            "\\hfill\n",
            "\\begin{minipage}[t]{0.4\\textwidth}\n",
            "\\centering\n",
            "\\includegraphics[width=0.9\\linewidth]{cifar10_lime/cifar10_truck_lime_explain_2.png}\n",
            "\\end{minipage}\n",
            "\\caption{Original and Lime explanation of CIFAR10 images. Images in left column are original images and images in the right column are LIME explanations.}\n",
            "\\end{figure}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}